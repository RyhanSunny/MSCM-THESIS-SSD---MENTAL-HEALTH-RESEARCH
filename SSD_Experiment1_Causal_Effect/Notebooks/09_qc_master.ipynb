{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Quality Control Notebook\n",
    "\n",
    "This notebook performs comprehensive quality checks on all pipeline outputs.\n",
    "\n",
    "## QC Checks:\n",
    "1. Row count consistency across datasets\n",
    "2. Missingness analysis\n",
    "3. Date consistency and logical ordering\n",
    "4. Duplicate ID checks\n",
    "5. Foreign key integrity\n",
    "6. Summary dashboard with PASS/FAIL status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Initialize results tracking\n",
    "qc_results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'checks': {},\n",
    "    'overall_status': 'PASS'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load All Datasets and Check Row Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define expected datasets\n",
    "datasets = {\n",
    "    'cohort': '../data_derived/cohort.parquet',\n",
    "    'exposure': '../data_derived/exposure.parquet',\n",
    "    'mediator': '../data_derived/mediator.parquet',\n",
    "    'outcomes': '../data_derived/outcomes.parquet',\n",
    "    'confounders': '../data_derived/confounders.parquet',\n",
    "    'patient_master': '../data_derived/patient_master.parquet',\n",
    "    'ps_weighted': '../data_derived/ps_weighted.parquet'\n",
    "}\n",
    "\n",
    "# Load datasets and check row counts\n",
    "loaded_data = {}\n",
    "row_counts = {}\n",
    "expected_rows = 250025  # From blueprint\n",
    "\n",
    "for name, path in datasets.items():\n",
    "    try:\n",
    "        df = pd.read_parquet(path)\n",
    "        loaded_data[name] = df\n",
    "        row_counts[name] = len(df)\n",
    "        print(f\"{name}: {len(df):,} rows\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"WARNING: {name} not found at {path}\")\n",
    "        row_counts[name] = 0\n",
    "\n",
    "# Check consistency\n",
    "row_count_check = all(count == expected_rows for count in row_counts.values() if count > 0)\n",
    "qc_results['checks']['row_count_consistency'] = {\n",
    "    'status': 'PASS' if row_count_check else 'FAIL',\n",
    "    'expected': expected_rows,\n",
    "    'actual': row_counts,\n",
    "    'consistent': row_count_check\n",
    "}\n",
    "\n",
    "if not row_count_check:\n",
    "    qc_results['overall_status'] = 'FAIL'\n",
    "    print(\"\\n‚ùå FAIL: Row counts are inconsistent!\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ PASS: All datasets have consistent row counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missingness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missingness in patient master table\n",
    "if 'patient_master' in loaded_data:\n",
    "    master_df = loaded_data['patient_master']\n",
    "    \n",
    "    # Calculate missing percentages\n",
    "    missing_pct = (master_df.isnull().sum() / len(master_df) * 100).sort_values(ascending=False)\n",
    "    missing_pct = missing_pct[missing_pct > 0]\n",
    "    \n",
    "    # Create missingness heatmap\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Select columns with any missing data\n",
    "    cols_with_missing = missing_pct.index[:20]  # Top 20\n",
    "    if len(cols_with_missing) > 0:\n",
    "        missing_matrix = master_df[cols_with_missing].isnull()\n",
    "        sns.heatmap(missing_matrix.T, cbar=True, yticklabels=True, \n",
    "                   cmap='RdYlBu', ax=ax)\n",
    "        ax.set_title('Missingness Heatmap (Top 20 Variables)')\n",
    "        ax.set_xlabel('Patient Index')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Check if missingness is acceptable (< 10% for key variables)\n",
    "    key_vars = ['Patient_ID', 'ssd_flag', 'age', 'sex_M']\n",
    "    key_missing = {var: missing_pct.get(var, 0) for var in key_vars}\n",
    "    \n",
    "    missingness_check = all(pct < 10 for pct in key_missing.values())\n",
    "    qc_results['checks']['missingness'] = {\n",
    "        'status': 'PASS' if missingness_check else 'FAIL',\n",
    "        'key_variables': key_missing,\n",
    "        'total_columns_with_missing': len(missing_pct),\n",
    "        'max_missing_pct': float(missing_pct.max()) if len(missing_pct) > 0 else 0\n",
    "    }\n",
    "    \n",
    "    if not missingness_check:\n",
    "        qc_results['overall_status'] = 'FAIL'\n",
    "    \n",
    "    print(f\"\\nMissingness Summary:\")\n",
    "    print(f\"Columns with missing data: {len(missing_pct)}\")\n",
    "    print(f\"Max missing percentage: {missing_pct.max():.1f}%\" if len(missing_pct) > 0 else \"No missing data\")\n",
    "    print(f\"\\nKey variable missingness:\")\n",
    "    for var, pct in key_missing.items():\n",
    "        print(f\"  {var}: {pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Date Consistency Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check date consistency\n",
    "date_issues = []\n",
    "\n",
    "if 'patient_master' in loaded_data:\n",
    "    df = loaded_data['patient_master']\n",
    "    \n",
    "    # Identify date columns\n",
    "    date_cols = [col for col in df.columns if 'date' in col.lower()]\n",
    "    \n",
    "    print(\"Date columns found:\", date_cols)\n",
    "    \n",
    "    # Convert to datetime if needed\n",
    "    for col in date_cols:\n",
    "        if df[col].dtype != 'datetime64[ns]':\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col])\n",
    "            except:\n",
    "                print(f\"Warning: Could not convert {col} to datetime\")\n",
    "    \n",
    "    # Check for future dates\n",
    "    today = pd.Timestamp.now()\n",
    "    for col in date_cols:\n",
    "        if df[col].dtype == 'datetime64[ns]':\n",
    "            future_dates = df[df[col] > today]\n",
    "            if len(future_dates) > 0:\n",
    "                date_issues.append(f\"{col} has {len(future_dates)} future dates\")\n",
    "    \n",
    "    # Check logical ordering\n",
    "    if 'index_date' in df.columns and 'death_date' in df.columns:\n",
    "        illogical = df[df['death_date'] < df['index_date']]\n",
    "        if len(illogical) > 0:\n",
    "            date_issues.append(f\"Death before index date: {len(illogical)} cases\")\n",
    "    \n",
    "    # Check study period consistency (2018-2020)\n",
    "    if 'index_date' in df.columns:\n",
    "        df['index_year'] = pd.to_datetime(df['index_date']).dt.year\n",
    "        year_counts = df['index_year'].value_counts().sort_index()\n",
    "        \n",
    "        plt.figure(figsize=(8, 5))\n",
    "        year_counts.plot(kind='bar')\n",
    "        plt.title('Patient Distribution by Index Year')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Number of Patients')\n",
    "        plt.axhline(y=expected_rows/3, color='r', linestyle='--', alpha=0.5, label='Expected if uniform')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Check if majority are in 2018-2020\n",
    "        study_period_pct = year_counts[year_counts.index.isin([2018, 2019, 2020])].sum() / len(df) * 100\n",
    "        if study_period_pct < 95:\n",
    "            date_issues.append(f\"Only {study_period_pct:.1f}% of patients in 2018-2020\")\n",
    "    \n",
    "    date_check = len(date_issues) == 0\n",
    "    qc_results['checks']['date_consistency'] = {\n",
    "        'status': 'PASS' if date_check else 'FAIL',\n",
    "        'issues': date_issues,\n",
    "        'study_period_coverage': study_period_pct if 'study_period_pct' in locals() else None\n",
    "    }\n",
    "    \n",
    "    if not date_check:\n",
    "        qc_results['overall_status'] = 'FAIL'\n",
    "        print(\"\\n‚ùå Date issues found:\")\n",
    "        for issue in date_issues:\n",
    "            print(f\"  - {issue}\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ PASS: All date checks passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Duplicate ID Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate Patient_IDs\n",
    "duplicate_issues = []\n",
    "\n",
    "for name, df in loaded_data.items():\n",
    "    if 'Patient_ID' in df.columns:\n",
    "        duplicates = df['Patient_ID'].duplicated().sum()\n",
    "        if duplicates > 0:\n",
    "            duplicate_issues.append(f\"{name}: {duplicates} duplicate Patient_IDs\")\n",
    "            print(f\"WARNING: {name} has {duplicates} duplicate Patient_IDs\")\n",
    "\n",
    "duplicate_check = len(duplicate_issues) == 0\n",
    "qc_results['checks']['duplicate_ids'] = {\n",
    "    'status': 'PASS' if duplicate_check else 'FAIL',\n",
    "    'issues': duplicate_issues\n",
    "}\n",
    "\n",
    "if not duplicate_check:\n",
    "    qc_results['overall_status'] = 'FAIL'\n",
    "    print(\"\\n‚ùå FAIL: Duplicate IDs found\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ PASS: No duplicate Patient_IDs found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Foreign Key Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all Patient_IDs exist in master cohort\n",
    "integrity_issues = []\n",
    "\n",
    "if 'cohort' in loaded_data:\n",
    "    master_ids = set(loaded_data['cohort']['Patient_ID'])\n",
    "    \n",
    "    for name, df in loaded_data.items():\n",
    "        if name != 'cohort' and 'Patient_ID' in df.columns:\n",
    "            dataset_ids = set(df['Patient_ID'])\n",
    "            \n",
    "            # Check for IDs not in master\n",
    "            orphan_ids = dataset_ids - master_ids\n",
    "            if len(orphan_ids) > 0:\n",
    "                integrity_issues.append(f\"{name}: {len(orphan_ids)} Patient_IDs not in cohort\")\n",
    "            \n",
    "            # Check for missing IDs\n",
    "            missing_ids = master_ids - dataset_ids\n",
    "            if len(missing_ids) > 0 and name != 'ps_weighted':  # ps_weighted might be filtered\n",
    "                print(f\"INFO: {name} missing {len(missing_ids)} patients from cohort\")\n",
    "\n",
    "integrity_check = len(integrity_issues) == 0\n",
    "qc_results['checks']['foreign_key_integrity'] = {\n",
    "    'status': 'PASS' if integrity_check else 'FAIL',\n",
    "    'issues': integrity_issues\n",
    "}\n",
    "\n",
    "if not integrity_check:\n",
    "    qc_results['overall_status'] = 'FAIL'\n",
    "    print(\"\\n‚ùå FAIL: Foreign key integrity issues\")\n",
    "    for issue in integrity_issues:\n",
    "        print(f\"  - {issue}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ PASS: All Patient_IDs properly linked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary dashboard\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"QUALITY CONTROL SUMMARY DASHBOARD\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nRun Date: {qc_results['timestamp']}\")\n",
    "print(f\"\\nOVERALL STATUS: {qc_results['overall_status']}\")\n",
    "print(\"\\nIndividual Checks:\")\n",
    "\n",
    "# Summary table\n",
    "check_summary = []\n",
    "for check_name, check_result in qc_results['checks'].items():\n",
    "    status = check_result['status']\n",
    "    icon = \"‚úÖ\" if status == \"PASS\" else \"‚ùå\"\n",
    "    check_summary.append({\n",
    "        'Check': check_name.replace('_', ' ').title(),\n",
    "        'Status': f\"{icon} {status}\",\n",
    "        'Details': len(check_result.get('issues', [])) if 'issues' in check_result else ''\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(check_summary)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save results to JSON\n",
    "output_path = Path('../results/qc_results.json')\n",
    "output_path.parent.mkdir(exist_ok=True)\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(qc_results, f, indent=2)\n",
    "print(f\"\\nQC results saved to: {output_path}\")\n",
    "\n",
    "# Final status\n",
    "if qc_results['overall_status'] == 'PASS':\n",
    "    print(\"\\nüéâ All quality checks PASSED! The pipeline outputs are consistent and valid.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Some quality checks FAILED. Please review the issues above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Checks: Data Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check key variable distributions\n",
    "if 'patient_master' in loaded_data:\n",
    "    df = loaded_data['patient_master']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Age distribution\n",
    "    if 'age' in df.columns:\n",
    "        df['age'].hist(bins=30, ax=axes[0,0], edgecolor='black')\n",
    "        axes[0,0].set_title('Age Distribution')\n",
    "        axes[0,0].set_xlabel('Age')\n",
    "        axes[0,0].axvline(df['age'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"age\"].mean():.1f}')\n",
    "        axes[0,0].legend()\n",
    "    \n",
    "    # Sex distribution\n",
    "    if 'sex_M' in df.columns:\n",
    "        sex_counts = df['sex_M'].value_counts()\n",
    "        sex_counts.plot(kind='bar', ax=axes[0,1])\n",
    "        axes[0,1].set_title('Sex Distribution')\n",
    "        axes[0,1].set_xticklabels(['Female', 'Male'], rotation=0)\n",
    "        axes[0,1].set_ylabel('Count')\n",
    "    \n",
    "    # SSD flag distribution\n",
    "    if 'ssd_flag' in df.columns:\n",
    "        ssd_counts = df['ssd_flag'].value_counts()\n",
    "        ssd_counts.plot(kind='bar', ax=axes[1,0])\n",
    "        axes[1,0].set_title('SSD Flag Distribution')\n",
    "        axes[1,0].set_xticklabels(['Control', 'SSD'], rotation=0)\n",
    "        axes[1,0].set_ylabel('Count')\n",
    "        \n",
    "        # Add percentage labels\n",
    "        total = len(df)\n",
    "        for i, v in enumerate(ssd_counts):\n",
    "            axes[1,0].text(i, v + total*0.01, f'{v/total*100:.1f}%', ha='center')\n",
    "    \n",
    "    # Charlson score distribution\n",
    "    if 'charlson_score' in df.columns:\n",
    "        df['charlson_score'].value_counts().sort_index().plot(kind='bar', ax=axes[1,1])\n",
    "        axes[1,1].set_title('Charlson Comorbidity Score Distribution')\n",
    "        axes[1,1].set_xlabel('Charlson Score')\n",
    "        axes[1,1].set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export notebook results for pipeline integration\n",
    "print(f\"\\nFinal QC Status for pipeline: {qc_results['overall_status']}\")\n",
    "\n",
    "# This output can be captured by papermill for automation\n",
    "qc_status = qc_results['overall_status']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}