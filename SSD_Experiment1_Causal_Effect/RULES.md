# AI Governance Rules: Python Healthcare Research
*Optimized for SSD/CPCSSN on Windows*

## üéØ Core Principles
```python
# Every script must follow
SEED = 42  # Reproducibility
DATA_DIR = Path(__file__).parent / 'data'  # No hardcoded paths
MAX_LINES_PER_FILE = 1500  # Stay under token limits
```

## üìÇ File Organization Rules

### **Stop File Explosion**
```python
# ‚ùå AI creates new files
analysis_v1.py
analysis_v2.py
analysis_final.py
analysis_final_FINAL.py

# ‚úÖ Use ONE file + git
analysis.py  # git handles versions
# Or if needed: analysis_20250315.py (ONE dated backup)
```

### **Module Size Limits**
```python
# Each .py file should be:
# - Under 1500 lines (‚âà20k tokens)
# - Single responsibility
# - Named by function, not version

# ‚ùå Bad structure
everything.py  # 5000 lines

# ‚úÖ Good structure
src/
  ‚îú‚îÄ‚îÄ data_loader.py      # ~500 lines: loading only
  ‚îú‚îÄ‚îÄ preprocessor.py     # ~800 lines: cleaning only  
  ‚îú‚îÄ‚îÄ feature_engineer.py # ~600 lines: feature creation
  ‚îî‚îÄ‚îÄ models.py          # ~700 lines: ML models
```

### **Documentation Control**
```python
# ‚ùå AI generates
README.md
README_updated.md
project_notes.md
analysis_notes_v3.md

# ‚úÖ Enforce structure
docs/
  ‚îú‚îÄ‚îÄ README.md          # Project overview ONLY
  ‚îú‚îÄ‚îÄ CHANGELOG.md       # All changes here
  ‚îî‚îÄ‚îÄ decisions.md       # Research decisions
  
# Everything else ‚Üí code comments
```

## üîç AI Search & Context Rules

### **Before Creating ANY File**
```python
# 1. Use SERENA MCP to search existing code
@ai_instruction
def before_coding():
    """
    ALWAYS run first:
    - Search for existing implementation
    - Check if function already exists
    - Find similar patterns
    """
    # Example: "Find all functions that process lab data"
    
# 2. Check context7 for latest patterns
@ai_instruction  
def check_documentation():
    """Use context7 to verify latest API/library usage"""
```

### **File Split Triggers**
```python
# When to split a file:
if any([
    line_count > 1500,
    class_count > 5,
    function_count > 20,
    mixed_responsibilities  # e.g., loading + modeling
]):
    split_file()

How to split:
Original: data_processor.py (2000 lines)
Becomes:
  ‚îú‚îÄ‚îÄ data_validator.py    # Input validation
  ‚îú‚îÄ‚îÄ data_transformer.py  # Transformations
  ‚îî‚îÄ‚îÄ data_aggregator.py   # Aggregations

## üö® Common AI Pitfalls ‚Üí Solutions

### 1. **Path Handling**
```python
# ‚ùå AI generates
df = pd.read_csv('C:\\Users\\data.csv')
# ‚úÖ Always fix to
df = pd.read_csv(Path('data') / 'data.csv')
```

### 2. **Memory Overflow**
```python
# ‚ùå AI loads everything
df = pd.read_csv('8GB_file.csv')
# ‚úÖ Chunk large files
for chunk in pd.read_csv('8GB_file.csv', chunksize=50000):
    process(chunk)
```

### 3. **Vague Exceptions**
```python
# ‚ùå AI's lazy handling
try: complex_op()
except: pass
# ‚úÖ Specific + actionable
try: 
    complex_op()
except pd.errors.ParserError as e:
    logger.error(f"CSV parsing failed: {e}")
    raise
```

### 4. **Missing Validation**
```python
# After EVERY data operation
assert not df.empty, "DataFrame empty after merge"
assert len(df) > 100, f"Only {len(df)} rows - check filters"
assert set(required_cols).issubset(df.columns), f"Missing: {set(required_cols) - set(df.columns)}"
```


## üìè Code Organization Rules

### **Import Management**
```python
# At the top of EVERY file
"""
Module purpose in one line.

Functions:
- load_data: Read CPCSSN files
- validate_schema: Check required columns
"""

# Standard library
import os
from pathlib import Path

# Third party
import pandas as pd
import numpy as np

# Local (relative imports)
from .config import Config
from .utils import safe_log
```

### **Function Limits**
```python
# Each function < 50 lines
# Each class < 200 lines
# Each file < 1500 lines

# ‚ùå Monster function
def analyze_everything(df):
    # 500 lines of code...
    
# ‚úÖ Decomposed
def load_ssd_data(path: Path) -> pd.DataFrame:
    """30 lines"""
    
def calculate_severity(df: pd.DataFrame) -> pd.Series:
    """40 lines"""
    
def generate_report(severity: pd.Series) -> None:
    """35 lines"""
```

### **Namespace Control**
```python
# ‚ùå Global namespace pollution
from utils import *
from config import *

# ‚úÖ Explicit imports
from utils import validate_dataframe, safe_log
from config import SYMPTOM_CODES, MIN_VISITS
```

## üîß Quick Fixes for AI Code

| AI Generates | You Fix To |
|--------------|------------|
| Creates `analysis_v2.py` | Update `analysis.py` + git commit |
| Giant 3000-line file | Split by responsibility |
| New README variant | Update existing `docs/README.md` |
| Inline magic numbers | Move to `config.py` |
| Recreates existing function | Search first with SERENA |
| Outdated library usage | Check context7 for latest |

## üìä Research-Specific Rules

### Config Pattern (No Magic Numbers)
```python
@dataclass
class Config:
    # DSM-5 thresholds
    MIN_ANXIETY_MONTHS: int = 6
    NORMAL_LAB_COUNT: int = 3
    # ICD-9 ranges
    SYMPTOM_CODES: range = range(780, 790)
    # Paths
    OUTPUT_DIR: Path = Path('output')
```

### Healthcare Privacy
```python
# Logging wrapper
def safe_log(msg, patient_id=None):
    """Never expose identifiers"""
    if patient_id:
        msg = msg.replace(str(patient_id), f"patient_{hash(patient_id) % 10000}")
    logger.info(msg)
```

## üßπ Refactoring Triggers

```python
# Automatic refactor when:
triggers = {
    'file_too_large': 'lines > 1500',
    'duplicate_code': 'same logic in 3+ places',
    'mixed_concerns': 'loading + analysis in same file',
    'version_explosion': 'file_v2.py exists',
    'unclear_purpose': 'no docstring or mixed functionality'
}

Refactor pattern:
1. Extract functions to utils.py
2. Split by data flow stage
3. Delete old versions
4. Update imports
```

## ‚ö° Performance + Organization

### Import Optimization
```python
# ‚ùå Import everything
import pandas as pd
import numpy as np  # Even if unused

# ‚úÖ Import only what's needed
from pandas import DataFrame, Series
from numpy import where, nan
```

### Lazy Loading
```python
# For optional heavy dependencies
def use_torch_model():
    import torch  # Only import when needed
    return torch.load('model.pt')
```

## üé® Final Rules

1. **One file per purpose** ‚Üí No v2, v3 versions
2. **Under 1500 lines** ‚Üí Split if larger
3. **Search before create** ‚Üí Use SERENA MCP
4. **Check latest docs** ‚Üí Use context7
5. **No duplicate files** ‚Üí Git handles versions
6. **Clear file names** ‚Üí `data_loader.py` not `stuff.py`
7. **Document in code** ‚Üí Not scattered .md files
8. **Archive don't duplicate** ‚Üí Old code to `/archive`
9. **Import explicitly** ‚Üí No `import *`
10. **Refactor early** ‚Üí Don't wait for 3000 lines

---
# Analysis Rules and Guidelines
**Personal commitment to research integrity**

## Core Principles

### 1. Never Fabricate or Assume Data
- **ALWAYS** trace every statistic back to its source
- **NEVER** report numbers without verifying where they came from
- **ALWAYS** distinguish between:
  - Actual measured data
  - Calculated/derived values (show the calculation)
  - Proxy estimates (explain the methodology)
  - Literature-based estimates (provide citation)

### 2. Transparency in Reporting
When reporting any statistic:
1. State the exact value
2. Explain how it was calculated/obtained
3. Note any limitations or assumptions
4. Provide verifiable source or calculation method

### 3. Data Verification Checklist
Before reporting any number, ask:
- [ ] Where did this come from?
- [ ] Can I show the exact calculation or source?
- [ ] Is this an estimate, proxy, or actual measurement?
- [ ] Have I clearly labeled what type of data this is?
- [ ] Can someone else verify this number?

### 4. Examples of Good Practice

**‚úÖ GOOD:**
"Medical costs averaged $425 per patient, calculated as proxy using Ontario billing codes: 4.74 visits √ó $75/visit + 0.38 referrals √ó $180/referral (Source: config.yaml lines 117-122)"

**‚ùå BAD:**
"Medical costs averaged $425 per patient"

**‚úÖ GOOD:**
"We identified 143,579 patients (55.9%) with SSD patterns based on our exposure criteria (see 02_exposure_flag.py output)"

**‚ùå BAD:**
"About half of patients showed SSD patterns"

### 5. Citation Requirements
Always provide:
- For data: File path and line numbers or query used
- For calculations: Formula and input values
- For literature: Full citation with page numbers
- For estimates: Methodology and assumptions

### 6. Ongoing Verification
- Re-check numbers when updating reports
- Document any changes in calculations
- Note when preliminary findings are updated
- Keep audit trail of all analyses

## Personal Accountability
I commit to following these rules throughout the analysis to ensure research integrity and reproducibility. Every claim will be backed by verifiable evidence.