{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a3fea80-b977-4bce-90d9-2354e44e8632",
   "metadata": {},
   "source": [
    "# CPCSSN Mental Health Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa40f9b8-4283-42c3-b479-b8f16872ac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import time\n",
    "\n",
    "# Import the local pipeline\n",
    "# from data_preparation import DataPreparationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e732037-fd55-4d42-89ba-7e37ad4b5cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2256b7cc-f2c3-4819-acf8-e441cf33e13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "{'data_paths': {'raw_data': 'extracted_data/', 'prepared_data': 'prepared_data/', 'file_extension': '.csv'}, 'use_gpu': False, 'chunk_size': 500000, 'tables': {'Patient': {'filename': 'C4MPatient', 'required_columns': ['Patient_ID', 'Sex', 'BirthYear', 'BirthMonth', 'OptedOut', 'OptOutDate'], 'date_columns': ['OptOutDate']}, 'PatientDemographic': {'filename': 'C4MPatientDemographic', 'required_columns': ['PatientDemographic_ID', 'Patient_ID', 'Network_ID', 'Site_ID', 'Cycle_ID', 'Occupation', 'HousingStatus', 'DateCreated'], 'date_columns': ['DateCreated']}, 'Encounter': {'filename': 'C4MEncounter', 'required_columns': ['Encounter_ID', 'Patient_ID', 'Network_ID', 'Site_ID', 'Provider_ID', 'EncounterDate', 'Reason_orig', 'Reason_calc'], 'date_columns': ['EncounterDate', 'DateCreated']}, 'EncounterDiagnosis': {'filename': 'C4MEncounterDiagnosis', 'required_columns': ['EncounterDiagnosis_ID', 'Network_ID', 'Site_ID', 'Patient_ID', 'Encounter_ID', 'Cycle_ID', 'DiagnosisText_orig', 'DiagnosisText_calc'], 'date_columns': ['DateCreated']}, 'FamilyHistory': {'filename': 'C4MFamilyHistory', 'required_columns': ['FamilyHistory_ID', 'Network_ID', 'Site_ID', 'Patient_ID', 'Encounter_ID', 'Relationship_orig', 'DateCreated'], 'date_columns': ['DateCreated']}, 'HealthCondition': {'filename': 'C4MHealthCondition', 'required_columns': ['HealthCondition_ID', 'Network_ID', 'Site_ID', 'Patient_ID', 'Encounter_ID', 'DiagnosisText_orig'], 'date_columns': ['DateCreated']}, 'Lab': {'filename': 'C4MLab', 'required_columns': ['Lab_ID', 'Network_ID', 'Site_ID', 'Patient_ID', 'TestResult_orig', 'PerformedDate'], 'date_columns': ['PerformedDate', 'DateCreated']}, 'MedicalProcedure': {'filename': 'C4MMedicalProcedure', 'required_columns': ['MedicalProcedure_ID', 'Network_ID', 'Site_ID', 'Patient_ID', 'PerformedDate'], 'date_columns': ['PerformedDate', 'DateCreated']}, 'Medication': {'filename': 'C4MMedication', 'required_columns': ['Medication_ID', 'Network_ID', 'Site_ID', 'Patient_ID', 'StartDate', 'StopDate'], 'date_columns': ['StartDate', 'StopDate', 'DateCreated']}, 'Referral': {'filename': 'C4MReferral', 'required_columns': ['Referral_ID', 'Network_ID', 'Site_ID', 'Patient_ID', 'Encounter_ID', 'CompletedDate'], 'date_columns': ['CompletedDate', 'DateCreated']}, 'RiskFactor': {'filename': 'C4MRiskFactor', 'required_columns': ['RiskFactor_ID', 'Network_ID', 'Site_ID', 'Patient_ID', 'StartDate', 'EndDate'], 'date_columns': ['StartDate', 'EndDate', 'DateCreated']}}, 'quality_thresholds': {'max_missing_pct': 0.3, 'min_date': '2020-01-01', 'max_date': '2025-12-31'}, 'sampling': {'n_patients': 2000, 'stratify_columns': ['Sex']}}\n",
      "Pipeline initialized.\n",
      "use_gpu = False\n",
      "chunk_size = 500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 00:57:00,975 - INFO - Loading table: Patient from extracted_data\\C4MPatient.csv\n",
      "2025-02-13 00:57:00,975 - INFO - Reading extracted_data\\C4MPatient.csv in chunks of size=500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading Tables ===\n",
      "\n",
      "Loading table: Patient\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 00:57:02,985 - INFO - Successfully loaded Patient: 352161 rows\n",
      "2025-02-13 00:57:03,083 - INFO - Loading table: PatientDemographic from extracted_data\\C4MPatientDemographic.csv\n",
      "2025-02-13 00:57:03,083 - INFO - Reading extracted_data\\C4MPatientDemographic.csv in chunks of size=500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Done: 352161 rows, 6 cols in 2.11s\n",
      "\n",
      "Loading table: PatientDemographic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 00:57:06,022 - INFO - Successfully loaded PatientDemographic: 352220 rows\n",
      "2025-02-13 00:57:06,022 - INFO - Loading table: Encounter from extracted_data\\C4MEncounter.csv\n",
      "2025-02-13 00:57:06,022 - INFO - Reading extracted_data\\C4MEncounter.csv in chunks of size=500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Done: 352220 rows, 15 cols in 2.94s\n",
      "\n",
      "Loading table: Encounter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 00:58:43,041 - INFO - Successfully loaded Encounter: 11577739 rows\n",
      "2025-02-13 00:58:43,042 - INFO - Loading table: EncounterDiagnosis from extracted_data\\C4MEncounterDiagnosis.csv\n",
      "2025-02-13 00:58:43,042 - INFO - Reading extracted_data\\C4MEncounterDiagnosis.csv in chunks of size=500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Done: 11577739 rows, 11 cols in 97.02s\n",
      "\n",
      "Loading table: EncounterDiagnosis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 00:58:46,009 - ERROR - Error loading EncounterDiagnosis: '|' expected after '\"'\n",
      "2025-02-13 00:58:46,094 - INFO - Loading table: FamilyHistory from extracted_data\\C4MFamilyHistory.csv\n",
      "2025-02-13 00:58:46,095 - INFO - Reading extracted_data\\C4MFamilyHistory.csv in chunks of size=500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  !! Error: '|' expected after '\"'\n",
      "\n",
      "Loading table: FamilyHistory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 00:58:49,507 - INFO - Successfully loaded FamilyHistory: 325202 rows\n",
      "2025-02-13 00:58:49,508 - INFO - Loading table: HealthCondition from extracted_data\\C4MHealthCondition.csv\n",
      "2025-02-13 00:58:49,509 - INFO - Reading extracted_data\\C4MHealthCondition.csv in chunks of size=500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Done: 325202 rows, 20 cols in 3.41s\n",
      "\n",
      "Loading table: HealthCondition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 00:58:50,173 - ERROR - Error loading HealthCondition: '|' expected after '\"'\n",
      "2025-02-13 00:58:50,207 - INFO - Loading table: Lab from extracted_data\\C4MLab.csv\n",
      "2025-02-13 00:58:50,207 - INFO - Reading extracted_data\\C4MLab.csv in chunks of size=500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  !! Error: '|' expected after '\"'\n",
      "\n",
      "Loading table: Lab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 00:58:56,847 - ERROR - Error loading Lab: '|' expected after '\"'\n",
      "2025-02-13 00:58:57,005 - INFO - Loading table: MedicalProcedure from extracted_data\\C4MMedicalProcedure.csv\n",
      "2025-02-13 00:58:57,005 - INFO - Reading extracted_data\\C4MMedicalProcedure.csv in chunks of size=500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  !! Error: '|' expected after '\"'\n",
      "\n",
      "Loading table: MedicalProcedure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 00:59:07,392 - INFO - Successfully loaded MedicalProcedure: 1203002 rows\n",
      "2025-02-13 00:59:07,394 - INFO - Loading table: Medication from extracted_data\\C4MMedication.csv\n",
      "2025-02-13 00:59:07,394 - INFO - Reading extracted_data\\C4MMedication.csv in chunks of size=500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Done: 1203002 rows, 10 cols in 10.39s\n",
      "\n",
      "Loading table: Medication\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 00:59:10,694 - ERROR - Error loading Medication: '|' expected after '\"'\n",
      "2025-02-13 00:59:10,823 - INFO - Loading table: Referral from extracted_data\\C4MReferral.csv\n",
      "2025-02-13 00:59:10,823 - INFO - Reading extracted_data\\C4MReferral.csv in chunks of size=500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  !! Error: '|' expected after '\"'\n",
      "\n",
      "Loading table: Referral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 00:59:20,731 - INFO - Successfully loaded Referral: 1141061 rows\n",
      "2025-02-13 00:59:20,731 - INFO - Loading table: RiskFactor from extracted_data\\C4MRiskFactor.csv\n",
      "2025-02-13 00:59:20,735 - INFO - Reading extracted_data\\C4MRiskFactor.csv in chunks of size=500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Done: 1141061 rows, 12 cols in 9.91s\n",
      "\n",
      "Loading table: RiskFactor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 00:59:21,659 - ERROR - Error loading RiskFactor: '|' expected after '\"'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  !! Error: '|' expected after '\"'\n",
      "\n",
      "Total load time: 140.73s\n",
      "\n",
      "=== Loading Summary ===\n",
      "                table      rows  columns  time_sec\n",
      "0             Patient    352161        6      2.11\n",
      "1  PatientDemographic    352220       15      2.94\n",
      "2           Encounter  11577739       11     97.02\n",
      "3       FamilyHistory    325202       20      3.41\n",
      "4    MedicalProcedure   1203002       10     10.39\n",
      "5            Referral   1141061       12      9.91\n"
     ]
    }
   ],
   "source": [
    "# 1) Check directories\n",
    "for folder in ['logs', 'prepared_data', 'extracted_data']:\n",
    "    Path(folder).mkdir(exist_ok=True)\n",
    "\n",
    "# 2) Load config\n",
    "config_path = Path('config.yaml')\n",
    "assert config_path.exists(), \"config.yaml not found!\"\n",
    "\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(config)\n",
    "\n",
    "# 3) Initialize pipeline\n",
    "pipeline = DataPreparationPipeline(\n",
    "    config_path=str(config_path),\n",
    "    data_dir=config['data_paths']['raw_data']\n",
    ")\n",
    "print(\"Pipeline initialized.\")\n",
    "print(f\"use_gpu = {pipeline.use_gpu}\")\n",
    "print(f\"chunk_size = {pipeline.chunk_size}\")\n",
    "\n",
    "# 4) Attempt to load data\n",
    "table_names = list(config['tables'].keys())\n",
    "tables = {}\n",
    "loading_log = []\n",
    "\n",
    "start_all = time.time()\n",
    "print(\"\\n=== Loading Tables ===\")\n",
    "for tname in table_names:\n",
    "    print(f\"\\nLoading table: {tname}\")\n",
    "    start_t = time.time()\n",
    "    try:\n",
    "        df = pipeline.load_table(tname)\n",
    "        tables[tname] = df\n",
    "        load_time = time.time() - start_t\n",
    "        loading_log.append({\n",
    "            \"table\": tname,\n",
    "            \"rows\": len(df),\n",
    "            \"columns\": len(df.columns),\n",
    "            \"time_sec\": round(load_time,2)\n",
    "        })\n",
    "        print(f\"  -> Done: {len(df)} rows, {len(df.columns)} cols in {load_time:.2f}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"  !! Error: {e}\")\n",
    "end_all = time.time()\n",
    "print(f\"\\nTotal load time: {round(end_all - start_all,2)}s\")\n",
    "\n",
    "# 5) Summarize loading\n",
    "load_report = pd.DataFrame(loading_log)\n",
    "print(\"\\n=== Loading Summary ===\")\n",
    "print(load_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80748187-7ad6-4dae-a2c8-0978a0de6ecb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cudf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcudf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(cudf\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cudf'"
     ]
    }
   ],
   "source": [
    "import cudf\n",
    "print(cudf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "175c410c-858f-4a0b-b1a6-3b6897969ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmed directory exists: logs\n",
      "Confirmed directory exists: prepared_data\n",
      "Confirmed directory exists: figures\n",
      "Confirmed directory exists: extracted_data\n"
     ]
    }
   ],
   "source": [
    "# Create necessary directories\n",
    "dirs_to_create = ['logs', 'prepared_data', 'figures', 'extracted_data']\n",
    "for dir_name in dirs_to_create:\n",
    "    Path(dir_name).mkdir(exist_ok=True)\n",
    "    print(f\"Confirmed directory exists: {dir_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "523d8739-e000-4a2a-b414-4a1a774e2f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded configuration\n",
      "\n",
      "Warning: The following files are missing:\n",
      "- C4MPatient.txt\n",
      "- C4MPatientDemographic.txt\n",
      "- C4MEncounter.txt\n",
      "- C4MEncounterDiagnosis.txt\n",
      "- C4MHealthCondition.txt\n",
      "- C4MLab.txt\n",
      "- C4MMedicalProcedure.txt\n",
      "- C4MMedication.txt\n",
      "- C4MRiskFactor.txt\n",
      "- C4MReferral.txt\n",
      "- C4MFamilyHistory.txt\n"
     ]
    }
   ],
   "source": [
    "# Verify config file exists before loading\n",
    "config_path = Path('config.yaml')\n",
    "if not config_path.exists():\n",
    "    raise FileNotFoundError(\"config.yaml not found in current directory\")\n",
    "\n",
    "# Load configuration with error handling\n",
    "try:\n",
    "    with open(config_path, 'r', encoding='utf-8') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    print(\"Successfully loaded configuration\")\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Error loading config.yaml: {str(e)}\")\n",
    "\n",
    "# Verify data directory structure\n",
    "data_dir = Path('extracted_data')\n",
    "if not data_dir.exists():\n",
    "    raise FileNotFoundError(\"extracted_data directory not found\")\n",
    "\n",
    "# List expected files based on config\n",
    "expected_files = [f\"{table_config['filename']}.txt\" \n",
    "                 for table_config in config['tables'].values()]\n",
    "\n",
    "# Check for missing files\n",
    "missing_files = [file for file in expected_files \n",
    "                if not (data_dir / file).exists()]\n",
    "\n",
    "if missing_files:\n",
    "    print(\"\\nWarning: The following files are missing:\")\n",
    "    for file in missing_files:\n",
    "        print(f\"- {file}\")\n",
    "else:\n",
    "    print(\"\\nAll expected data files are present\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5553cca-f35f-4bf0-94ae-ec3a045fd5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully initialized data preparation pipeline\n",
      "\n",
      "Configuration Summary:\n",
      "--------------------------------------------------\n",
      "Number of tables configured: 11\n",
      "Sampling size: 2000 patients\n",
      "Data paths:\n",
      "- raw_data: extracted_data/\n",
      "- prepared_data: prepared_data/\n",
      "- file_extension: .csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize pipeline with error handling\n",
    "try:\n",
    "    pipeline = DataPreparationPipeline(\n",
    "        config_path=str(config_path),\n",
    "        data_dir=str(data_dir)\n",
    "    )\n",
    "    print(\"\\nSuccessfully initialized data preparation pipeline\")\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Error initializing pipeline: {str(e)}\")\n",
    "\n",
    "# Print initial configuration summary\n",
    "print(\"\\nConfiguration Summary:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Number of tables configured: {len(config['tables'])}\")\n",
    "print(f\"Sampling size: {config['sampling']['n_patients']} patients\")\n",
    "print(f\"Data paths:\")\n",
    "for key, path in config['data_paths'].items():\n",
    "    print(f\"- {key}: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3032a39f-9825-44bd-ba79-ad8fce974a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table Availability Status:\n",
      "--------------------------------------------------\n",
      "Patient: Available\n",
      "PatientDemographic: Available\n",
      "Encounter: Available\n",
      "EncounterDiagnosis: Available\n",
      "HealthCondition: Available\n",
      "Lab: Available\n",
      "MedicalProcedure: Available\n",
      "Medication: Available\n",
      "RiskFactor: Available\n",
      "Referral: Available\n",
      "FamilyHistory: Available\n",
      "\n",
      "Initialization complete - ready to proceed with data loading\n"
     ]
    }
   ],
   "source": [
    "# Function to check table availability\n",
    "def check_table_exists(table_name: str) -> bool:\n",
    "    \"\"\"Check if a table file exists in the data directory.\"\"\"\n",
    "    filename = config['tables'][table_name]['filename']\n",
    "    file_ext = config['data_paths']['file_extension']\n",
    "    return (data_dir / f\"{filename}{file_ext}\").exists()\n",
    "\n",
    "# Print table availability status\n",
    "print(\"\\nTable Availability Status:\")\n",
    "print(\"-\" * 50)\n",
    "for table_name in config['tables']:\n",
    "    status = \"Available\" if check_table_exists(table_name) else \"Missing\"\n",
    "    print(f\"{table_name}: {status}\")\n",
    "\n",
    "print(\"\\nInitialization complete - ready to proceed with data loading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b152f7-bcd0-4b67-a41a-f04f0534dbd1",
   "metadata": {},
   "source": [
    "## Data Load and Initial Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5f4f1a6-7aca-4b99-89b4-0bb748b3c282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 00:28:20,009 - INFO - Loading table: Patient from extracted_data\\C4MPatient.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tables...\n",
      "\n",
      "Loading Patient...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 00:28:26,020 - ERROR - Error loading Patient: No columns to parse from file\n",
      "2025-02-13 00:28:26,022 - INFO - Loading table: PatientDemographic from extracted_data\\C4MPatientDemographic.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading Patient: No columns to parse from file\n",
      "\n",
      "Loading PatientDemographic...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 00:28:32,120 - ERROR - Error loading PatientDemographic: No columns to parse from file\n",
      "2025-02-13 00:28:32,120 - INFO - Loading table: Encounter from extracted_data\\C4MEncounter.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading PatientDemographic: No columns to parse from file\n",
      "\n",
      "Loading Encounter...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 00:31:42,663 - ERROR - Error loading Encounter: No columns to parse from file\n",
      "2025-02-13 00:31:42,663 - INFO - Loading table: EncounterDiagnosis from extracted_data\\C4MEncounterDiagnosis.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading Encounter: No columns to parse from file\n",
      "\n",
      "Loading EncounterDiagnosis...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Load table\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m tables[table_name] \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mload_table(table_name)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Calculate loading statistics\u001b[39;00m\n\u001b[0;32m     28\u001b[0m duration \u001b[38;5;241m=\u001b[39m (datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time)\u001b[38;5;241m.\u001b[39mtotal_seconds()\n",
      "File \u001b[1;32m~\\Documents\\CPCSSN Datasets Care4Mind\\New Extraction Feb 2025\\data_preparation.py:69\u001b[0m, in \u001b[0;36mDataPreparationPipeline.load_table\u001b[1;34m(self, table_name)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading table: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# Read CSV (assumes comma delimiter by default)\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m'\u001b[39m, on_bad_lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# Basic cleaning\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_basic_cleaning(df, table_config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:133\u001b[0m, in \u001b[0;36mPythonParser.__init__\u001b[1;34m(self, f, **kwds)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_col_indices: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    128\u001b[0m columns: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Scalar \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m]]\n\u001b[0;32m    129\u001b[0m (\n\u001b[0;32m    130\u001b[0m     columns,\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_original_columns,\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols,\n\u001b[1;32m--> 133\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_columns()\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Now self.columns has the set of columns that we will process.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# The original set is stored in self.original_columns.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'index_names'\u001b[39;00m\n\u001b[0;32m    138\u001b[0m (\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns,\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_names,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_names,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    146\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:404\u001b[0m, in \u001b[0;36mPythonParser._infer_columns\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m level, hr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(header):\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 404\u001b[0m         line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffered_line()\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_pos \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m hr:\n\u001b[0;32m    407\u001b[0m             line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_line()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:637\u001b[0m, in \u001b[0;36mPythonParser._buffered_line\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_line()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:738\u001b[0m, in \u001b[0;36mPythonParser._next_line\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 738\u001b[0m     orig_line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_iter_line(row_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m orig_line \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:805\u001b[0m, in \u001b[0;36mPythonParser._next_iter_line\u001b[1;34m(self, row_num)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;66;03m# assert for mypy, data is Iterator[str] or None, would error in next\u001b[39;00m\n\u001b[0;32m    804\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 805\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(line, \u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[1;32m<frozen codecs>:319\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from data_preparation import DataPreparationPipeline\n",
    "import yaml\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# 1. Data Loading\n",
    "print(\"Loading tables...\")\n",
    "tables = {}\n",
    "loading_summary = []\n",
    "\n",
    "for table_name in config['tables'].keys():\n",
    "    try:\n",
    "        print(f\"\\nLoading {table_name}...\")\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Load table\n",
    "        tables[table_name] = pipeline.load_table(table_name)\n",
    "        \n",
    "        # Calculate loading statistics\n",
    "        duration = (datetime.now() - start_time).total_seconds()\n",
    "        memory_usage = tables[table_name].memory_usage(deep=True).sum() / 1024**2  # MB\n",
    "        \n",
    "        loading_summary.append({\n",
    "            'Table': table_name,\n",
    "            'Rows': len(tables[table_name]),\n",
    "            'Columns': len(tables[table_name].columns),\n",
    "            'Memory (MB)': f\"{memory_usage:.2f}\",\n",
    "            'Load Time (s)': f\"{duration:.2f}\",\n",
    "            'Status': 'Success'\n",
    "        })\n",
    "        \n",
    "        print(f\"Loaded {len(tables[table_name])} rows and {len(tables[table_name].columns)} columns\")\n",
    "        print(f\"Memory usage: {memory_usage:.2f} MB\")\n",
    "        print(f\"Loading time: {duration:.2f} seconds\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {table_name}: {str(e)}\")\n",
    "        loading_summary.append({\n",
    "            'Table': table_name,\n",
    "            'Rows': 0,\n",
    "            'Columns': 0,\n",
    "            'Memory (MB)': 0,\n",
    "            'Load Time (s)': 0,\n",
    "            'Status': f'Failed: {str(e)}'\n",
    "        })\n",
    "\n",
    "# Display loading summary\n",
    "summary_df = pd.DataFrame(loading_summary)\n",
    "print(\"\\nLoading Summary:\")\n",
    "print(\"=\" * 80)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# 2. Initial Data Quality Check\n",
    "print(\"\\nChecking data quality...\")\n",
    "quality_summary = []\n",
    "\n",
    "for table_name, df in tables.items():\n",
    "    # Calculate basic statistics\n",
    "    missing_pct = df.isnull().mean().mean() * 100\n",
    "    duplicate_rows = df.duplicated().sum()\n",
    "    memory_usage = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    \n",
    "    # Get date columns for this table\n",
    "    date_cols = config['tables'][table_name].get('date_columns', [])\n",
    "    date_range = {}\n",
    "    \n",
    "    # Check date ranges if date columns exist\n",
    "    for col in date_cols:\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col])\n",
    "                date_range[col] = {\n",
    "                    'min': df[col].min(),\n",
    "                    'max': df[col].max()\n",
    "                }\n",
    "            except Exception as e:\n",
    "                date_range[col] = f\"Error converting dates: {str(e)}\"\n",
    "    \n",
    "    quality_summary.append({\n",
    "        'Table': table_name,\n",
    "        'Missing Data %': f\"{missing_pct:.2f}%\",\n",
    "        'Duplicate Rows': duplicate_rows,\n",
    "        'Memory (MB)': f\"{memory_usage:.2f}\",\n",
    "        'Date Ranges': date_range\n",
    "    })\n",
    "\n",
    "print(\"\\nQuality Summary:\")\n",
    "print(\"=\" * 80)\n",
    "for entry in quality_summary:\n",
    "    print(f\"\\nTable: {entry['Table']}\")\n",
    "    print(f\"Missing Data: {entry['Missing Data %']}\")\n",
    "    print(f\"Duplicate Rows: {entry['Duplicate Rows']}\")\n",
    "    print(f\"Memory Usage: {entry['Memory (MB']} MB\")\n",
    "    if entry['Date Ranges']:\n",
    "        print(\"Date Ranges:\")\n",
    "        for col, range_info in entry['Date Ranges'].items():\n",
    "            print(f\"  {col}: {range_info}\")\n",
    "\n",
    "# 3. Sample size verification\n",
    "print(\"\\nVerifying patient sample size...\")\n",
    "if 'Patient' in tables:\n",
    "    patient_count = len(tables['Patient'])\n",
    "    print(f\"Total patients in dataset: {patient_count}\")\n",
    "    \n",
    "    if 'sampling' in config and 'n_patients' in config['sampling']:\n",
    "        target_sample = config['sampling']['n_patients']\n",
    "        print(f\"Target sample size: {target_sample}\")\n",
    "        if patient_count > target_sample:\n",
    "            print(\"Will proceed with sampling in next step\")\n",
    "        else:\n",
    "            print(\"Warning: Current patient count is less than target sample size\")\n",
    "\n",
    "# 4. Save initial statistics\n",
    "loading_stats = pd.DataFrame(loading_summary)\n",
    "loading_stats.to_csv('prepared_data/loading_statistics.csv', index=False)\n",
    "\n",
    "print(\"\\nInitial data assessment complete. Ready to proceed with sampling and detailed analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a59afc-6cca-4dcf-aa1b-02356ef5565c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
