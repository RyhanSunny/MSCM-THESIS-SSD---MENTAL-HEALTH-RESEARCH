{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Driven Identification and Management of Somatic Symptom Disorder in Primary Care\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SSD identification pipeline...\n",
      "GPU available: NVIDIA RTX A1000 6GB Laptop GPU\n",
      "\n",
      "Loading and preprocessing CPCSSN data...\n",
      "Loading PatientDemographic_merged from prepared_data/PatientDemographic_merged_prepared.csv...\n",
      "Successfully loaded 1000 rows from PatientDemographic_merged\n",
      "Sampled 995 patients for analysis\n",
      "Analysis period: 2023-02-26 to 2025-02-25\n",
      "Loading HealthCondition from prepared_data/HealthCondition_prepared.csv...\n",
      "Successfully loaded 2571583 rows from HealthCondition\n",
      "Loading Encounter from prepared_data/Encounter_prepared.csv...\n",
      "Successfully loaded 11577739 rows from Encounter\n",
      "Loading EncounterDiagnosis from prepared_data/EncounterDiagnosis_prepared.csv...\n",
      "Successfully loaded 12471764 rows from EncounterDiagnosis\n",
      "Encounter DataFrame columns: ['Encounter_ID', 'Network_ID', 'Site_ID', 'Patient_ID', 'Provider_ID', 'Cycle_ID', 'EncounterDate', 'Reason_orig', 'Reason_calc', 'EncounterType', 'DateCreated']\n",
      "EncounterDiagnosis DataFrame columns: ['EncounterDiagnosis_ID', 'Network_ID', 'Site_ID', 'Patient_ID', 'Encounter_ID', 'Cycle_ID', 'DiagnosisText_orig', 'DiagnosisText_calc', 'DiagnosisCodeType_orig', 'DiagnosisCodeType_calc', 'DiagnosisCode_orig', 'DiagnosisCode_calc', 'DateCreated', 'orphan_encounter']\n",
      "Joining on columns: ['Encounter_ID', 'Patient_ID']\n",
      "Loading Lab from prepared_data/Lab_prepared.csv...\n",
      "Successfully loaded 8528807 rows from Lab\n",
      "Loading Referral from prepared_data/Referral_prepared.csv...\n",
      "Successfully loaded 1141061 rows from Referral\n",
      "Loading Medication from prepared_data/Medication_prepared.csv...\n",
      "Successfully loaded 7706628 rows from Medication\n",
      "Loading MedicalProcedure from prepared_data/MedicalProcedure_prepared.csv...\n",
      "Successfully loaded 1203002 rows from MedicalProcedure\n",
      "\n",
      "Basic statistics:\n",
      "Total patients: 995\n",
      "Age distribution: count    995.000000\n",
      "mean      62.838191\n",
      "std       22.123463\n",
      "min       18.000000\n",
      "25%       45.000000\n",
      "50%       65.000000\n",
      "75%       78.500000\n",
      "max      109.000000\n",
      "Name: Age, dtype: float64\n",
      "Gender distribution: Sex\n",
      "FEMALE    733\n",
      "MALE      262\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "# AI-Driven Identification and Management of Somatic Symptom Disorder in Primary Care\n",
    "# Part 1: Environment Setup, Data Loading and Preprocessing\n",
    "\n",
    "# This implementation is specifically designed for the CPCSSN dataset as described\n",
    "# in the Data YAML and Data Description documents.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Starting SSD identification pipeline...\")\n",
    "\n",
    "#################################\n",
    "# 1. Environment Setup\n",
    "#################################\n",
    "\n",
    "# Check for GPU availability\n",
    "try:\n",
    "    import torch\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    if gpu_available:\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"GPU not available, using CPU\")\n",
    "except ImportError:\n",
    "    gpu_available = False\n",
    "    device = \"cpu\"\n",
    "    print(\"PyTorch not installed, using CPU\")\n",
    "\n",
    "#################################\n",
    "# 2. Data Loading Configuration\n",
    "#################################\n",
    "\n",
    "# Define paths based on YAML configuration\n",
    "RAW_DATA_PATH = \"extracted_data/\"\n",
    "PREPARED_DATA_PATH = \"prepared_data/\"\n",
    "FILE_EXTENSION = \".csv\"\n",
    "CHUNK_SIZE = 500000  # For processing large files\n",
    "\n",
    "# Define tables to load based on YAML\n",
    "TABLES = {\n",
    "    'Encounter': 'Encounter_prepared',\n",
    "    'EncounterDiagnosis': 'EncounterDiagnosis_prepared',\n",
    "    'HealthCondition': 'HealthCondition_prepared',\n",
    "    'Lab': 'Lab_prepared',\n",
    "    'Medication': 'Medication_prepared',\n",
    "    'MedicalProcedure': 'MedicalProcedure_prepared',\n",
    "    'Referral': 'Referral_prepared',\n",
    "    'RiskFactor': 'RiskFactor_prepared',\n",
    "    'PatientDemographic_merged': 'PatientDemographic_merged_prepared'\n",
    "}\n",
    "\n",
    "# Define date columns for each table based on YAML\n",
    "DATE_COLUMNS = {\n",
    "    'Encounter': ['EncounterDate', 'DateCreated'],\n",
    "    'EncounterDiagnosis': ['DateCreated'],\n",
    "    'HealthCondition': ['DateCreated'],\n",
    "    'Lab': ['PerformedDate', 'DateCreated'],\n",
    "    'MedicalProcedure': ['PerformedDate', 'DateCreated'],\n",
    "    'Medication': ['StartDate', 'StopDate', 'DateCreated'],\n",
    "    'Referral': ['CompletedDate', 'DateCreated'],\n",
    "    'RiskFactor': ['StartDate', 'EndDate', 'DateCreated'],\n",
    "    'PatientDemographic_merged': ['DateCreated']\n",
    "}\n",
    "\n",
    "# Specific SSD-related ICD-9 codes based on validated DSM-IV to DSM-5 crosswalks\n",
    "SSD_SPECIFIC_ICD9_CODES = [\n",
    "    '300.81',  # Somatization Disorder (in DSM-5, encompassed by SSD)\n",
    "    '300.82',  # Undifferentiated Somatoform Disorder / SSD / Unspecified somatic symptom disorder\n",
    "    '300.7',   # Hypochondriasis/Illness Anxiety Disorder\n",
    "    '300.11',  # Conversion Disorder (Functional Neurological Symptom Disorder)\n",
    "    '307.80',  # Pain Disorder associated with psychological factors\n",
    "    '307.89',  # Pain Disorder with both psychological factors and medical condition\n",
    "    '316',     # Psychological Factors Affecting Other Medical Conditions\n",
    "    '300.16',  # Factitious Disorder (predominantly psychological signs)\n",
    "    '300.19',  # Factitious Disorder (predominantly physical signs)\n",
    "    '301.51',  # Chronic factitious illness with physical symptoms\n",
    "    '300.89'   # Other Specified Somatic Symptom and Related Disorders\n",
    "]\n",
    "\n",
    "# Broader mental health and somatic symptom ICD-9 code ranges\n",
    "MENTAL_HEALTH_ICD9_RANGES = [\n",
    "    (290, 319),  # Primary mental health codes\n",
    "    (327, 327),  # Sleep disorders\n",
    "    (331, 333),  # Neurological conditions\n",
    "    (780, 780),  # Symptoms involving nervous and musculoskeletal systems\n",
    "    (786, 788),  # Symptoms involving respiratory, digestive, urinary systems\n",
    "    (799, 799)   # Other ill-defined conditions\n",
    "]\n",
    "\n",
    "# Common somatic symptom keywords organized by context\n",
    "# Clinical Documentation Terminology (from physician notes)\n",
    "CLINICAL_DOCUMENTATION_KEYWORDS = [\n",
    "    'medically unexplained symptoms', 'no organic cause', 'no medical explanation',\n",
    "    'disproportionate to findings', 'somatic complaints', 'psychosomatic', 'psychogenic',\n",
    "    'functional neurological symptom', 'PNES', 'pseudoseizures', 'conversion disorder',\n",
    "    'health anxiety', 'illness preoccupation despite reassurance', 'frequent flyer',\n",
    "    'doctor shopping', 'symptom magnification', 'symptom amplification',\n",
    "    'out of proportion to examination', 'high healthcare utilization',\n",
    "    'multiple negative workups', 'extensive negative workup', 'normal tests',\n",
    "    'benign exam', 'reassurance ineffective', 'multiple somatic symptoms'\n",
    "]\n",
    "\n",
    "# Patient-Reported Symptom Descriptions\n",
    "PATIENT_REPORT_KEYWORDS = [\n",
    "    'constant pain but doctors can\\'t find anything', 'tired all the time',\n",
    "    'exhausted all the time', 'whole body hurts', 'pain all over',\n",
    "    'headaches every day and nothing helps', 'short of breath but tests normal',\n",
    "    'worried I have a serious disease', 'no one understands how bad I feel',\n",
    "    'doctors must have missed something', 'I know something is wrong',\n",
    "    'keep checking my body', 'medicine doesn\\'t make me better',\n",
    "    'sensitive to medications', 'stress makes symptoms worse',\n",
    "    'symptoms across multiple systems', 'headaches stomach pains nausea fatigue',\n",
    "    'multiple sites of pain', 'symptoms come and go', 'been to many doctors'\n",
    "]\n",
    "\n",
    "# NLP-Oriented Phrases and Patterns\n",
    "NLP_ORIENTED_KEYWORDS = [\n",
    "    'medically unexplained physical symptoms', 'MUPS', 'multiple unexplained symptoms',\n",
    "    'symptoms with no clear etiology', 'persistent somatic complaints despite normal workup',\n",
    "    'excessive health-related behaviors', 'illness worry not alleviated by medical reassurance',\n",
    "    'high health anxiety', 'functional symptom disorder', 'conversion symptoms',\n",
    "    'somatic focus', 'somatic preoccupation', 'negative diagnostic cascade',\n",
    "    'denies relief after reassurance', 'symptom fixation', 'symptom vigilance',\n",
    "    'health-related fear', 'excessive time and energy on health concerns',\n",
    "    'catastrophizing health symptoms', 'heightened body awareness'\n",
    "]\n",
    "\n",
    "# Consolidated list of all somatic symptom keywords for text analysis\n",
    "SOMATIC_KEYWORDS = (CLINICAL_DOCUMENTATION_KEYWORDS + \n",
    "                   PATIENT_REPORT_KEYWORDS + \n",
    "                   NLP_ORIENTED_KEYWORDS)\n",
    "\n",
    "#################################\n",
    "# 3. Data Loading Functions\n",
    "#################################\n",
    "\n",
    "def load_table(table_name, date_columns=None, nrows=None):\n",
    "    \"\"\"\n",
    "    Load a CPCSSN table with proper date parsing\n",
    "    \n",
    "    Args:\n",
    "        table_name (str): The name of the table\n",
    "        date_columns (list): List of columns containing dates\n",
    "        nrows (int): Number of rows to load (None for all)\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: The loaded table\n",
    "    \"\"\"\n",
    "    filename = TABLES[table_name] + FILE_EXTENSION\n",
    "    filepath = os.path.join(PREPARED_DATA_PATH, filename)\n",
    "    \n",
    "    print(f\"Loading {table_name} from {filepath}...\")\n",
    "    \n",
    "    # Define date parser for the date columns\n",
    "    if date_columns:\n",
    "        date_parser = lambda x: pd.to_datetime(x, errors='coerce')\n",
    "        parse_dates = date_columns\n",
    "    else:\n",
    "        date_parser = None\n",
    "        parse_dates = None\n",
    "    \n",
    "    # Try to load the file\n",
    "    try:\n",
    "        if nrows:\n",
    "            df = pd.read_csv(filepath, \n",
    "                             parse_dates=parse_dates, \n",
    "                             date_parser=date_parser,\n",
    "                             nrows=nrows)\n",
    "        else:\n",
    "            # For large files, use chunking\n",
    "            df = pd.read_csv(filepath, \n",
    "                             parse_dates=parse_dates, \n",
    "                             date_parser=date_parser,\n",
    "                             chunksize=CHUNK_SIZE)\n",
    "            # Concatenate chunks\n",
    "            df = pd.concat(df)\n",
    "            \n",
    "        print(f\"Successfully loaded {len(df)} rows from {table_name}\")\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {table_name}: {e}\")\n",
    "        # Return empty DataFrame with expected columns\n",
    "        return pd.DataFrame(columns=[])\n",
    "\n",
    "#################################\n",
    "# 4. Data Preprocessing Functions\n",
    "#################################\n",
    "\n",
    "def preprocess_patient_demographics(df):\n",
    "    \"\"\"\n",
    "    Preprocess the PatientDemographic_merged table\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): The raw patient demographics table\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Preprocessed patient demographics\n",
    "    \"\"\"\n",
    "    if len(df) == 0:\n",
    "        return df\n",
    "    \n",
    "    # Keep only essential columns\n",
    "    essential_cols = [\n",
    "        'PatientDemographic_ID', 'Patient_ID', 'Network_ID', 'Site_ID',\n",
    "        'Sex', 'BirthYear', 'BirthMonth', 'ResidencePostalCode',\n",
    "        'PatientStatus_calc', 'OptedOut'\n",
    "    ]\n",
    "    \n",
    "    # Only keep patients who have not opted out\n",
    "    df = df[df['OptedOut'] == 0]\n",
    "    \n",
    "    # Filter columns\n",
    "    df = df[essential_cols]\n",
    "    \n",
    "    # Calculate age (approximate)\n",
    "    current_year = datetime.now().year\n",
    "    df['Age'] = current_year - df['BirthYear']\n",
    "    \n",
    "    # Filter to adults only (18+)\n",
    "    df = df[df['Age'] >= 18]\n",
    "    \n",
    "    # Convert categorical variables\n",
    "    df['Sex'] = df['Sex'].astype('category')\n",
    "    \n",
    "    # Extract first 3 digits of postal code for regional analysis\n",
    "    if 'ResidencePostalCode' in df.columns:\n",
    "        df['Region'] = df['ResidencePostalCode'].astype(str).str.slice(0, 3)\n",
    "        df['Region'] = df['Region'].astype('category')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def identify_mental_health_conditions(df):\n",
    "    \"\"\"\n",
    "    Identify patients with mental health conditions from HealthCondition table,\n",
    "    with specific focus on SSD and related conditions\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): The HealthCondition table\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame with Patient_ID and mental health flags\n",
    "    \"\"\"\n",
    "    if len(df) == 0:\n",
    "        return pd.DataFrame(columns=['Patient_ID', 'has_mental_health_condition', 'has_ssd_related_code'])\n",
    "    \n",
    "    # Initialize result DataFrame\n",
    "    mental_health_patients = pd.DataFrame()\n",
    "    \n",
    "    # Function to check if code is in mental health range\n",
    "    def is_mental_health_code(code):\n",
    "        if pd.isna(code) or not code:\n",
    "            return False\n",
    "        \n",
    "        # First check specific SSD codes (exact matching)\n",
    "        if str(code).strip() in SSD_SPECIFIC_ICD9_CODES:\n",
    "            return True\n",
    "        \n",
    "        # Then check range-based codes\n",
    "        try:\n",
    "            code_int = int(float(code))\n",
    "            for start, end in MENTAL_HEALTH_ICD9_RANGES:\n",
    "                if start <= code_int <= end:\n",
    "                    return True\n",
    "            return False\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    # Function to check if code is specifically a SSD-related code\n",
    "    def is_ssd_related_code(code):\n",
    "        if pd.isna(code) or not code:\n",
    "            return False\n",
    "        \n",
    "        return str(code).strip() in SSD_SPECIFIC_ICD9_CODES\n",
    "    \n",
    "    # Check DiagnosisCode_calc column for mental health codes\n",
    "    if 'DiagnosisCode_calc' in df.columns:\n",
    "        df['is_mental_health'] = df['DiagnosisCode_calc'].apply(is_mental_health_code)\n",
    "        df['is_ssd_related'] = df['DiagnosisCode_calc'].apply(is_ssd_related_code)\n",
    "        \n",
    "        # Also check for mental health keywords in DiagnosisText_calc\n",
    "        mental_health_keywords = [\n",
    "            'depression', 'anxiety', 'mental', 'psychiatric', 'psychological',\n",
    "            'mood', 'somatoform', 'somatic symptom', 'hypochondriasis', 'illness anxiety'\n",
    "        ]\n",
    "        \n",
    "        # Additional SSD-specific keywords\n",
    "        ssd_specific_keywords = [\n",
    "            'somatic symptom disorder', 'somatization', 'somatoform', 'psychosomatic', \n",
    "            'conversion disorder', 'functional neurological', 'illness anxiety',\n",
    "            'hypochondriasis', 'factitious disorder', 'pain disorder',\n",
    "            'medically unexplained', 'psychological factors affecting'\n",
    "        ]\n",
    "        \n",
    "        if 'DiagnosisText_calc' in df.columns:\n",
    "            # General mental health check\n",
    "            text_pattern = '|'.join(mental_health_keywords)\n",
    "            df['has_mental_health_text'] = df['DiagnosisText_calc'].astype(str).str.contains(\n",
    "                text_pattern, case=False, na=False\n",
    "            )\n",
    "            \n",
    "            # SSD-specific text check\n",
    "            ssd_text_pattern = '|'.join(ssd_specific_keywords)\n",
    "            df['has_ssd_text'] = df['DiagnosisText_calc'].astype(str).str.contains(\n",
    "                ssd_text_pattern, case=False, na=False\n",
    "            )\n",
    "            \n",
    "            # Combine code and text indicators\n",
    "            df['has_mental_health'] = df['is_mental_health'] | df['has_mental_health_text']\n",
    "            df['has_ssd_related'] = df['is_ssd_related'] | df['has_ssd_text']\n",
    "        else:\n",
    "            df['has_mental_health'] = df['is_mental_health']\n",
    "            df['has_ssd_related'] = df['is_ssd_related']\n",
    "        \n",
    "        # Get unique patients with mental health conditions and SSD conditions\n",
    "        mental_health_patients = df[df['has_mental_health']][['Patient_ID', 'has_ssd_related']].drop_duplicates()\n",
    "        mental_health_patients['has_mental_health_condition'] = 1\n",
    "        \n",
    "        # Rename for clarity\n",
    "        mental_health_patients = mental_health_patients.rename(\n",
    "            columns={'has_ssd_related': 'has_ssd_related_code'}\n",
    "        )\n",
    "        \n",
    "        # Fill missing values\n",
    "        mental_health_patients['has_ssd_related_code'] = mental_health_patients['has_ssd_related_code'].fillna(0).astype(int)\n",
    "    \n",
    "    return mental_health_patients\n",
    "\n",
    "def calculate_patient_encounter_stats(encounter_df, encounter_diag_df, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Calculate patient encounter statistics for a specific time period\n",
    "    \n",
    "    Args:\n",
    "        encounter_df (pandas.DataFrame): The Encounter table\n",
    "        encounter_diag_df (pandas.DataFrame): The EncounterDiagnosis table\n",
    "        start_date (datetime): Start date for the analysis period\n",
    "        end_date (datetime): End date for the analysis period\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Patient-level encounter statistics\n",
    "    \"\"\"\n",
    "    if len(encounter_df) == 0 or len(encounter_diag_df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Print column names for debugging\n",
    "    print(f\"Encounter DataFrame columns: {encounter_df.columns.tolist()}\")\n",
    "    print(f\"EncounterDiagnosis DataFrame columns: {encounter_diag_df.columns.tolist()}\")\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    encounter_df_copy = encounter_df.copy()\n",
    "    encounter_diag_df_copy = encounter_diag_df.copy()\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    required_encounter_cols = ['Encounter_ID', 'Patient_ID', 'EncounterDate']\n",
    "    required_diag_cols = ['Encounter_ID', 'Patient_ID', 'DiagnosisCode_calc']\n",
    "    \n",
    "    # Check if all required columns exist in encounter_df\n",
    "    if not all(col in encounter_df_copy.columns for col in required_encounter_cols):\n",
    "        print(f\"Warning: Missing required columns in Encounter table. Required: {required_encounter_cols}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Check if all required columns exist in encounter_diag_df\n",
    "    if not all(col in encounter_diag_df_copy.columns for col in required_diag_cols):\n",
    "        print(f\"Warning: Missing required columns in EncounterDiagnosis table. Required: {required_diag_cols}\")\n",
    "        if 'DiagnosisCode_calc' not in encounter_diag_df_copy.columns:\n",
    "            # If we don't have diagnosis codes, just return encounter counts\n",
    "            if 'EncounterDate' in encounter_df_copy.columns:\n",
    "                valid_encounters = encounter_df_copy[\n",
    "                    (encounter_df_copy['EncounterDate'] >= start_date) & \n",
    "                    (encounter_df_copy['EncounterDate'] <= end_date)\n",
    "                ]\n",
    "            else:\n",
    "                valid_encounters = encounter_df_copy\n",
    "                \n",
    "            encounter_counts = valid_encounters.groupby('Patient_ID').size().reset_index(name='encounter_count')\n",
    "            return encounter_counts\n",
    "    \n",
    "    # Filter encounters within the date range\n",
    "    if 'EncounterDate' in encounter_df_copy.columns:\n",
    "        valid_encounters = encounter_df_copy[\n",
    "            (encounter_df_copy['EncounterDate'] >= start_date) & \n",
    "            (encounter_df_copy['EncounterDate'] <= end_date)\n",
    "        ]\n",
    "    else:\n",
    "        valid_encounters = encounter_df_copy\n",
    "    \n",
    "    # Count encounters per patient\n",
    "    encounter_counts = valid_encounters.groupby('Patient_ID').size().reset_index(name='encounter_count')\n",
    "    \n",
    "    # Ensure we have Encounter_ID in the diagnosis data\n",
    "    if 'Encounter_ID' not in encounter_diag_df_copy.columns:\n",
    "        # If we don't have encounter linkage, just return encounter counts\n",
    "        return encounter_counts\n",
    "    \n",
    "    # Join with encounter diagnosis to get diagnosis info\n",
    "    # Use a common set of join columns that are definitely in both DataFrames\n",
    "    join_columns = []\n",
    "    for col in ['Encounter_ID', 'Patient_ID']:\n",
    "        if col in valid_encounters.columns and col in encounter_diag_df_copy.columns:\n",
    "            join_columns.append(col)\n",
    "    \n",
    "    if not join_columns:\n",
    "        print(\"Warning: No common columns to join Encounter and EncounterDiagnosis tables\")\n",
    "        return encounter_counts\n",
    "    \n",
    "    # Print the join columns for debugging\n",
    "    print(f\"Joining on columns: {join_columns}\")\n",
    "    \n",
    "    # Perform the join to get diagnosis info, ensuring Patient_ID is preserved\n",
    "    encounter_with_diag = pd.merge(\n",
    "        valid_encounters[join_columns],\n",
    "        encounter_diag_df_copy,\n",
    "        on=join_columns,\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Function to check if code is a symptom code (780-789 range)\n",
    "    def is_symptom_code(code):\n",
    "        if pd.isna(code) or not code:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            code_int = int(float(code))\n",
    "            return 780 <= code_int <= 789\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    # Identify symptom-related diagnoses\n",
    "    if 'DiagnosisCode_calc' in encounter_with_diag.columns:\n",
    "        encounter_with_diag['is_symptom_code'] = encounter_with_diag['DiagnosisCode_calc'].apply(is_symptom_code)\n",
    "        \n",
    "        # Check if Patient_ID exists in the merged DataFrame\n",
    "        if 'Patient_ID' not in encounter_with_diag.columns:\n",
    "            print(\"Warning: Patient_ID missing after merge. Cannot group by Patient_ID.\")\n",
    "            return encounter_counts\n",
    "        \n",
    "        # Count symptom codes per patient\n",
    "        symptom_df = encounter_with_diag[encounter_with_diag['is_symptom_code']]\n",
    "        if len(symptom_df) > 0:\n",
    "            symptom_counts = symptom_df.groupby('Patient_ID').size().reset_index(\n",
    "                name='symptom_code_count'\n",
    "            )\n",
    "            \n",
    "            # Merge encounter counts with symptom counts\n",
    "            patient_stats = pd.merge(encounter_counts, symptom_counts, on='Patient_ID', how='left')\n",
    "            patient_stats['symptom_code_count'] = patient_stats['symptom_code_count'].fillna(0)\n",
    "            \n",
    "            # Calculate ratio of symptom codes to total encounters\n",
    "            patient_stats['symptom_encounter_ratio'] = patient_stats['symptom_code_count'] / patient_stats['encounter_count']\n",
    "            \n",
    "            return patient_stats\n",
    "        else:\n",
    "            # No symptom codes found, just add a zero column\n",
    "            encounter_counts['symptom_code_count'] = 0\n",
    "            encounter_counts['symptom_encounter_ratio'] = 0\n",
    "            return encounter_counts\n",
    "    else:\n",
    "        return encounter_counts\n",
    "\n",
    "def get_lab_test_patterns(lab_df, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Analyze patterns in lab tests for patients\n",
    "    \n",
    "    Args:\n",
    "        lab_df (pandas.DataFrame): The Lab table\n",
    "        start_date (datetime): Start date for the analysis period\n",
    "        end_date (datetime): End date for the analysis period\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Patient-level lab test statistics\n",
    "    \"\"\"\n",
    "    if len(lab_df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Filter lab tests within date range\n",
    "    if 'PerformedDate' in lab_df.columns:\n",
    "        valid_labs = lab_df[\n",
    "            (lab_df['PerformedDate'] >= start_date) & \n",
    "            (lab_df['PerformedDate'] <= end_date)\n",
    "        ]\n",
    "    else:\n",
    "        valid_labs = lab_df\n",
    "    \n",
    "    # Count total lab tests per patient\n",
    "    lab_counts = valid_labs.groupby('Patient_ID').size().reset_index(name='total_lab_tests')\n",
    "    \n",
    "    # Identify normal results\n",
    "    # Approach 1: Check if result is within normal range\n",
    "    if all(col in valid_labs.columns for col in ['TestResult_calc', 'LowerNormal', 'UpperNormal']):\n",
    "        try:\n",
    "            # Convert numeric columns\n",
    "            valid_labs['TestResult_num'] = pd.to_numeric(valid_labs['TestResult_calc'], errors='coerce')\n",
    "            valid_labs['LowerNormal_num'] = pd.to_numeric(valid_labs['LowerNormal'], errors='coerce')\n",
    "            valid_labs['UpperNormal_num'] = pd.to_numeric(valid_labs['UpperNormal'], errors='coerce')\n",
    "            \n",
    "            # Check if result is within normal range\n",
    "            valid_labs['is_normal'] = (\n",
    "                (valid_labs['TestResult_num'] >= valid_labs['LowerNormal_num']) & \n",
    "                (valid_labs['TestResult_num'] <= valid_labs['UpperNormal_num'])\n",
    "            )\n",
    "        except:\n",
    "            # Fall back to text-based approach\n",
    "            valid_labs['is_normal'] = valid_labs['TestResult_calc'].astype(str).str.contains(\n",
    "                'normal|negative|unremarkable', case=False, na=False\n",
    "            )\n",
    "    # Approach 2: Look for keywords in result text\n",
    "    else:\n",
    "        valid_labs['is_normal'] = valid_labs['TestResult_calc'].astype(str).str.contains(\n",
    "            'normal|negative|unremarkable', case=False, na=False\n",
    "        )\n",
    "    \n",
    "    # Count normal lab tests per patient\n",
    "    normal_counts = valid_labs[valid_labs['is_normal']].groupby('Patient_ID').size().reset_index(\n",
    "        name='normal_lab_tests'\n",
    "    )\n",
    "    \n",
    "    # Merge lab counts with normal counts\n",
    "    lab_stats = pd.merge(lab_counts, normal_counts, on='Patient_ID', how='left')\n",
    "    lab_stats['normal_lab_tests'] = lab_stats['normal_lab_tests'].fillna(0)\n",
    "    \n",
    "    # Calculate ratio of normal to total tests\n",
    "    lab_stats['normal_test_ratio'] = lab_stats['normal_lab_tests'] / lab_stats['total_lab_tests']\n",
    "    \n",
    "    # Calculate distinct test types per patient\n",
    "    test_diversity = valid_labs.groupby('Patient_ID')['Name_calc'].nunique().reset_index(\n",
    "        name='unique_test_types'\n",
    "    )\n",
    "    \n",
    "    # Merge with lab stats\n",
    "    lab_stats = pd.merge(lab_stats, test_diversity, on='Patient_ID', how='left')\n",
    "    \n",
    "    return lab_stats\n",
    "\n",
    "def analyze_referral_patterns(referral_df, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Analyze patterns in referrals for patients\n",
    "    \n",
    "    Args:\n",
    "        referral_df (pandas.DataFrame): The Referral table\n",
    "        start_date (datetime): Start date for the analysis period\n",
    "        end_date (datetime): End date for the analysis period\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Patient-level referral statistics\n",
    "    \"\"\"\n",
    "    if len(referral_df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Filter referrals within date range\n",
    "    if 'CompletedDate' in referral_df.columns:\n",
    "        valid_referrals = referral_df[\n",
    "            (referral_df['CompletedDate'] >= start_date) & \n",
    "            (referral_df['CompletedDate'] <= end_date)\n",
    "        ]\n",
    "    else:\n",
    "        valid_referrals = referral_df\n",
    "    \n",
    "    # Count total referrals per patient\n",
    "    referral_counts = valid_referrals.groupby('Patient_ID').size().reset_index(name='total_referrals')\n",
    "    \n",
    "    # Get unique specialists per patient\n",
    "    specialist_diversity = valid_referrals.groupby('Patient_ID')['Name_calc'].nunique().reset_index(\n",
    "        name='unique_specialists'\n",
    "    )\n",
    "    \n",
    "    # Merge referral counts with specialist diversity\n",
    "    referral_stats = pd.merge(referral_counts, specialist_diversity, on='Patient_ID', how='left')\n",
    "    \n",
    "    return referral_stats\n",
    "\n",
    "def analyze_medication_patterns(medication_df, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Analyze patterns in medications for patients\n",
    "    \n",
    "    Args:\n",
    "        medication_df (pandas.DataFrame): The Medication table\n",
    "        start_date (datetime): Start date for the analysis period\n",
    "        end_date (datetime): End date for the analysis period\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Patient-level medication statistics\n",
    "    \"\"\"\n",
    "    if len(medication_df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Filter medications within date range\n",
    "    if 'StartDate' in medication_df.columns:\n",
    "        valid_medications = medication_df[\n",
    "            (medication_df['StartDate'] >= start_date) | \n",
    "            ((medication_df['StopDate'] >= start_date) & (medication_df['StopDate'] <= end_date)) |\n",
    "            pd.isna(medication_df['StopDate'])\n",
    "        ]\n",
    "    else:\n",
    "        valid_medications = medication_df\n",
    "    \n",
    "    # Count total medications per patient\n",
    "    med_counts = valid_medications.groupby('Patient_ID').size().reset_index(name='total_medications')\n",
    "    \n",
    "    # Identify psychotropic medications\n",
    "    psychotropic_keywords = [\n",
    "        'antidepressant', 'ssri', 'snri', 'anxiolytic', 'benzodiazepine',\n",
    "        'antipsychotic', 'mood stabilizer', 'prozac', 'zoloft', 'paxil',\n",
    "        'celexa', 'lexapro', 'effexor', 'cymbalta', 'wellbutrin', 'xanax',\n",
    "        'ativan', 'klonopin', 'valium', 'risperdal', 'zyprexa', 'seroquel',\n",
    "        'lithium', 'depakote', 'lamictal'\n",
    "    ]\n",
    "    \n",
    "    if 'Name_calc' in valid_medications.columns:\n",
    "        text_pattern = '|'.join(psychotropic_keywords)\n",
    "        valid_medications['is_psychotropic'] = valid_medications['Name_calc'].astype(str).str.contains(\n",
    "            text_pattern, case=False, na=False\n",
    "        )\n",
    "        \n",
    "        # Count psychotropic medications per patient\n",
    "        psychotropic_counts = valid_medications[valid_medications['is_psychotropic']].groupby('Patient_ID').size().reset_index(\n",
    "            name='psychotropic_count'\n",
    "        )\n",
    "        \n",
    "        # Merge medication counts with psychotropic counts\n",
    "        med_stats = pd.merge(med_counts, psychotropic_counts, on='Patient_ID', how='left')\n",
    "        med_stats['psychotropic_count'] = med_stats['psychotropic_count'].fillna(0)\n",
    "        \n",
    "        # Calculate ratio of psychotropic to total medications\n",
    "        med_stats['psychotropic_ratio'] = med_stats['psychotropic_count'] / med_stats['total_medications']\n",
    "        \n",
    "        return med_stats\n",
    "    else:\n",
    "        return med_counts\n",
    "\n",
    "def analyze_text_fields(df, text_column, keyword_categories=None):\n",
    "    \"\"\"\n",
    "    Analyze text fields for somatic symptom keywords with detailed categorization\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing text column\n",
    "        text_column (str): Name of the column containing text\n",
    "        keyword_categories (dict): Dictionary of keyword categories. If None, uses default categories\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with Patient_ID and keyword counts by category\n",
    "    \"\"\"\n",
    "    if len(df) == 0 or text_column not in df.columns:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # If no categories provided, use defaults\n",
    "    if keyword_categories is None:\n",
    "        keyword_categories = {\n",
    "            'clinical_documentation': CLINICAL_DOCUMENTATION_KEYWORDS,\n",
    "            'patient_report': PATIENT_REPORT_KEYWORDS, \n",
    "            'nlp_oriented': NLP_ORIENTED_KEYWORDS\n",
    "        }\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Create a column for total keyword count across all categories\n",
    "    df_copy['total_keyword_count'] = 0\n",
    "    \n",
    "    # Process each keyword category separately\n",
    "    for category, keywords in keyword_categories.items():\n",
    "        # Create a regex pattern for this category - make it case insensitive\n",
    "        pattern = '|'.join([re.escape(kw.lower()) for kw in keywords])\n",
    "        \n",
    "        # Count occurrences of keywords in text for this category\n",
    "        category_col = f'keywords_{category}_count'\n",
    "        \n",
    "        # Convert to lowercase for case-insensitive matching and then count\n",
    "        df_copy[category_col] = df_copy[text_column].astype(str).str.lower().str.count(pattern)\n",
    "        \n",
    "        # Add to total count\n",
    "        df_copy['total_keyword_count'] += df_copy[category_col]\n",
    "        \n",
    "        # Also flag if any keywords from this category are present\n",
    "        df_copy[f'has_{category}_keywords'] = df_copy[category_col] > 0\n",
    "    \n",
    "    # Calculate patient-level aggregates\n",
    "    agg_dict = {\n",
    "        'total_keyword_count': 'sum',\n",
    "        text_column: 'count'  # Count total documents\n",
    "    }\n",
    "    \n",
    "    # Add aggregations for each category\n",
    "    for category in keyword_categories.keys():\n",
    "        agg_dict[f'keywords_{category}_count'] = 'sum'\n",
    "        agg_dict[f'has_{category}_keywords'] = 'sum'\n",
    "    \n",
    "    # Group by patient\n",
    "    result = df_copy.groupby('Patient_ID').agg(agg_dict).reset_index()\n",
    "    \n",
    "    # Rename the document count column\n",
    "    result = result.rename(columns={text_column: 'total_documents'})\n",
    "    \n",
    "    # Calculate proportion of documents with ANY keywords\n",
    "    result['documents_with_keywords'] = (df_copy['total_keyword_count'] > 0).groupby(df_copy['Patient_ID']).sum().values\n",
    "    result['keyword_document_ratio'] = result['documents_with_keywords'] / result['total_documents']\n",
    "    \n",
    "    # Calculate proportion for each category\n",
    "    for category in keyword_categories.keys():\n",
    "        result[f'{category}_document_ratio'] = result[f'has_{category}_keywords'] / result['total_documents']\n",
    "    \n",
    "    # Calculate the enrichment score - a measure of how concentrated the somatic symptom language is\n",
    "    # Higher values indicate more somatic language per document, which may correlate with SSD severity\n",
    "    result['somatic_language_density'] = result['total_keyword_count'] / result['total_documents']\n",
    "    \n",
    "    return result\n",
    "\n",
    "#################################\n",
    "# 5. Main Data Loading Function\n",
    "#################################\n",
    "\n",
    "def load_and_preprocess_cpcssn_data(sample_size=None):\n",
    "    \"\"\"\n",
    "    Load and preprocess all necessary CPCSSN tables for SSD analysis\n",
    "    \n",
    "    Args:\n",
    "        sample_size (int): Number of patients to sample (None for all)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of DataFrames and stats ready for analysis\n",
    "    \"\"\"\n",
    "    print(\"\\nLoading and preprocessing CPCSSN data...\")\n",
    "    \n",
    "    # Load patient demographics first\n",
    "    patient_df = load_table('PatientDemographic_merged', \n",
    "                          date_columns=DATE_COLUMNS['PatientDemographic_merged'],\n",
    "                          nrows=sample_size)\n",
    "    \n",
    "    # Basic preprocessing of patient data\n",
    "    patient_df = preprocess_patient_demographics(patient_df)\n",
    "    \n",
    "    # If we want a sample, filter to those patient IDs only\n",
    "    if sample_size and len(patient_df) > 0:\n",
    "        patient_sample = patient_df.sample(min(sample_size, len(patient_df)), random_state=42)\n",
    "        patient_ids = patient_sample['Patient_ID'].unique()\n",
    "        print(f\"Sampled {len(patient_ids)} patients for analysis\")\n",
    "    else:\n",
    "        patient_ids = patient_df['Patient_ID'].unique() if len(patient_df) > 0 else []\n",
    "    \n",
    "    # Define analysis period (e.g., last 2 years)\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=730)  # 2 years\n",
    "    \n",
    "    print(f\"Analysis period: {start_date.date()} to {end_date.date()}\")\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    results = {\n",
    "        'patients': patient_df,\n",
    "        'analysis_period': {\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Load and process other tables if we have patient data\n",
    "    if len(patient_df) > 0:\n",
    "        \n",
    "        # Load HealthCondition table and identify mental health conditions\n",
    "        health_condition_df = load_table('HealthCondition', \n",
    "                                       date_columns=DATE_COLUMNS['HealthCondition'],\n",
    "                                       nrows=None)\n",
    "        \n",
    "        # Filter to our patient sample if applicable\n",
    "        if len(patient_ids) > 0:\n",
    "            health_condition_df = health_condition_df[health_condition_df['Patient_ID'].isin(patient_ids)]\n",
    "        \n",
    "        # Identify mental health conditions with specific focus on SSD-related codes\n",
    "        mental_health_df = identify_mental_health_conditions(health_condition_df)\n",
    "        results['mental_health'] = mental_health_df\n",
    "        \n",
    "        # Load Encounter and EncounterDiagnosis tables\n",
    "        encounter_df = load_table('Encounter', \n",
    "                                date_columns=DATE_COLUMNS['Encounter'],\n",
    "                                nrows=None)\n",
    "        \n",
    "        encounter_diag_df = load_table('EncounterDiagnosis', \n",
    "                                      date_columns=DATE_COLUMNS['EncounterDiagnosis'],\n",
    "                                      nrows=None)\n",
    "        \n",
    "        # Filter to our patient sample if applicable\n",
    "        if len(patient_ids) > 0:\n",
    "            encounter_df = encounter_df[encounter_df['Patient_ID'].isin(patient_ids)]\n",
    "            encounter_diag_df = encounter_diag_df[encounter_diag_df['Patient_ID'].isin(patient_ids)]\n",
    "        \n",
    "        # Calculate encounter statistics\n",
    "        encounter_stats = calculate_patient_encounter_stats(\n",
    "            encounter_df, encounter_diag_df, start_date, end_date\n",
    "        )\n",
    "        results['encounter_stats'] = encounter_stats\n",
    "        \n",
    "        # Analyze text fields for SSD-related terminology\n",
    "        # First check chief complaint in Encounter table\n",
    "        if 'Reason_orig' in encounter_df.columns:\n",
    "            reason_text_analysis = analyze_text_fields(encounter_df, 'Reason_orig')\n",
    "            results['reason_text_analysis'] = reason_text_analysis\n",
    "        \n",
    "        # Also analyze DiagnosisText fields in EncounterDiagnosis for more context\n",
    "        if 'DiagnosisText_orig' in encounter_diag_df.columns:\n",
    "            diagnosis_text_analysis = analyze_text_fields(encounter_diag_df, 'DiagnosisText_orig')\n",
    "            results['diagnosis_text_analysis'] = diagnosis_text_analysis\n",
    "        \n",
    "        # And check DiagnosisText in HealthCondition for chronic conditions\n",
    "        if 'DiagnosisText_orig' in health_condition_df.columns:\n",
    "            condition_text_analysis = analyze_text_fields(health_condition_df, 'DiagnosisText_orig')\n",
    "            results['condition_text_analysis'] = condition_text_analysis\n",
    "        \n",
    "        # Load and analyze Lab tests\n",
    "        lab_df = load_table('Lab', \n",
    "                          date_columns=DATE_COLUMNS['Lab'],\n",
    "                          nrows=None)\n",
    "        \n",
    "        if len(patient_ids) > 0:\n",
    "            lab_df = lab_df[lab_df['Patient_ID'].isin(patient_ids)]\n",
    "        \n",
    "        lab_stats = get_lab_test_patterns(lab_df, start_date, end_date)\n",
    "        results['lab_stats'] = lab_stats\n",
    "        \n",
    "        # Load and analyze Referrals\n",
    "        referral_df = load_table('Referral', \n",
    "                               date_columns=DATE_COLUMNS['Referral'],\n",
    "                               nrows=None)\n",
    "        \n",
    "        if len(patient_ids) > 0:\n",
    "            referral_df = referral_df[referral_df['Patient_ID'].isin(patient_ids)]\n",
    "        \n",
    "        referral_stats = analyze_referral_patterns(referral_df, start_date, end_date)\n",
    "        results['referral_stats'] = referral_stats\n",
    "        \n",
    "        # Load and analyze Medications\n",
    "        medication_df = load_table('Medication', \n",
    "                                 date_columns=DATE_COLUMNS['Medication'],\n",
    "                                 nrows=None)\n",
    "        \n",
    "        if len(patient_ids) > 0:\n",
    "            medication_df = medication_df[medication_df['Patient_ID'].isin(patient_ids)]\n",
    "        \n",
    "        medication_stats = analyze_medication_patterns(medication_df, start_date, end_date)\n",
    "        results['medication_stats'] = medication_stats\n",
    "        \n",
    "        # Load MedicalProcedure to check for multiple diagnostic procedures\n",
    "        procedure_df = load_table('MedicalProcedure',\n",
    "                                date_columns=DATE_COLUMNS['MedicalProcedure'],\n",
    "                                nrows=None)\n",
    "        \n",
    "        if len(patient_ids) > 0:\n",
    "            procedure_df = procedure_df[procedure_df['Patient_ID'].isin(patient_ids)]\n",
    "        \n",
    "        # Analyze procedures - count total procedures and diagnostic procedures per patient\n",
    "        if len(procedure_df) > 0:\n",
    "            # Identify diagnostic procedures vs treatments\n",
    "            diagnostic_keywords = [\n",
    "                'endoscopy', 'biopsy', 'x-ray', 'xray', 'mri', 'ct', 'scan', \n",
    "                'ultrasound', 'echo', 'ecg', 'ekg', 'imaging', 'diagnostic', \n",
    "                'examination', 'assessment', 'test', 'evaluation'\n",
    "            ]\n",
    "            \n",
    "            if 'Name_calc' in procedure_df.columns:\n",
    "                # Create pattern for matching diagnostic procedures\n",
    "                diagnostic_pattern = '|'.join(diagnostic_keywords)\n",
    "                procedure_df['is_diagnostic'] = procedure_df['Name_calc'].astype(str).str.contains(\n",
    "                    diagnostic_pattern, case=False, na=False\n",
    "                )\n",
    "                \n",
    "                # Group by patient and count\n",
    "                procedure_stats = procedure_df.groupby('Patient_ID').agg(\n",
    "                    total_procedures=('MedicalProcedure_ID', 'count'),\n",
    "                    diagnostic_procedures=('is_diagnostic', 'sum')\n",
    "                ).reset_index()\n",
    "                \n",
    "                # Calculate ratio of diagnostic to total procedures\n",
    "                procedure_stats['diagnostic_ratio'] = procedure_stats['diagnostic_procedures'] / procedure_stats['total_procedures']\n",
    "                \n",
    "                results['procedure_stats'] = procedure_stats\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage: Load a small sample for development\n",
    "if __name__ == \"__main__\":\n",
    "    # For development, use a small sample\n",
    "    sample_size = 1000  # Set to None for all patients\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    data = load_and_preprocess_cpcssn_data(sample_size=sample_size)\n",
    "    \n",
    "    # Print some basic statistics\n",
    "    print(\"\\nBasic statistics:\")\n",
    "    \n",
    "    if 'patients' in data and len(data['patients']) > 0:\n",
    "        print(f\"Total patients: {len(data['patients'])}\")\n",
    "        print(f\"Age distribution: {data['patients']['Age'].describe()}\")\n",
    "        print(f\"Gender distribution: {data['patients']['Sex'].value_counts()}\")\n",
    "    \n",
    "    if 'encounter_stats' in data and len(data['encounter_stats']) > 0:\n",
    "        print(f\"\\nEncounter statistics:\")\n",
    "        print(f\"Average encounters per patient: {data['encounter_stats']['encounter_count'].mean():.2f}\")\n",
    "        if 'symptom_code_count' in data['encounter_stats'].columns:\n",
    "            print(f\"Average symptom codes per patient: {data['encounter_stats']['symptom_code_count'].mean():.2f}\")\n",
    "        \n",
    "    if 'lab_stats' in data and len(data['lab_stats']) > 0:\n",
    "        print(f\"\\nLab test statistics:\")\n",
    "        print(f\"Average lab tests per patient: {data['lab_stats']['total_lab_tests'].mean():.2f}\")\n",
    "        if 'normal_test_ratio' in data['lab_stats'].columns:\n",
    "            print(f\"Average normal test ratio: {data['lab_stats']['normal_test_ratio'].mean():.2f}\")\n",
    "    \n",
    "    print(\"\\nData preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feature Engineering and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll start with creating a master feature engineering function that will integrate all the patient data into a unified dataset with SSD-relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shap\n",
      "  Downloading shap-0.46.0-cp312-cp312-win_amd64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\projectc4m\\anaconda3\\lib\\site-packages (from shap) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\projectc4m\\anaconda3\\lib\\site-packages (from shap) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\projectc4m\\anaconda3\\lib\\site-packages (from shap) (1.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\projectc4m\\anaconda3\\lib\\site-packages (from shap) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\projectc4m\\anaconda3\\lib\\site-packages (from shap) (4.66.5)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\projectc4m\\anaconda3\\lib\\site-packages (from shap) (24.1)\n",
      "Collecting slicer==0.0.8 (from shap)\n",
      "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: numba in c:\\users\\projectc4m\\anaconda3\\lib\\site-packages (from shap) (0.60.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\projectc4m\\anaconda3\\lib\\site-packages (from shap) (3.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\projectc4m\\anaconda3\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\projectc4m\\anaconda3\\lib\\site-packages (from numba->shap) (0.43.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\projectc4m\\anaconda3\\lib\\site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\projectc4m\\anaconda3\\lib\\site-packages (from pandas->shap) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\projectc4m\\anaconda3\\lib\\site-packages (from pandas->shap) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\projectc4m\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\projectc4m\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\projectc4m\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
      "Downloading shap-0.46.0-cp312-cp312-win_amd64.whl (456 kB)\n",
      "Downloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: slicer, shap\n",
      "Successfully installed shap-0.46.0 slicer-0.0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~etuptools (C:\\Users\\ProjectC4M\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~etuptools (C:\\Users\\ProjectC4M\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~etuptools (C:\\Users\\ProjectC4M\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Part 2: Feature Engineering and Data Integration"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Research Questions and Hypothesis\n",
       "\n",
       "**Primary Research Question:**  \n",
       "Can we identify patients with probable Somatic Symptom Disorder (SSD) using EMR data from Canadian primary care practices?\n",
       "\n",
       "**Secondary Questions:**  \n",
       "1. What EMR features most strongly correlate with SSD patterns?\n",
       "2. How does healthcare utilization differ between patients with high vs. low somatic symptom burden?\n",
       "3. Can we identify clusters or subtypes within the SSD phenotype?\n",
       "\n",
       "**Hypothesis:**  \n",
       "We hypothesize that patients with SSD patterns will show:\n",
       "1. Higher frequency of primary care visits\n",
       "2. Multiple unexplained symptoms coded across body systems\n",
       "3. Higher proportion of normal lab results\n",
       "4. More frequent referrals to specialists\n",
       "5. Presence of mental health comorbidities (especially anxiety)\n",
       "6. Evidence of somatic language in clinical notes\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Feature Engineering Plan\n",
       "\n",
       "We will transform the preprocessed data into a unified patient-level feature set for analysis.\n",
       "Our engineering approach includes:\n",
       "\n",
       "1. **Basic Utilization Features**: Visit counts, visit frequency, lab test counts\n",
       "2. **SSD-Specific Features**: SSD codes, unexplained symptom patterns, normal test ratios\n",
       "3. **Text-Derived Features**: Somatic language markers in clinical notes \n",
       "4. **Temporal Pattern Features**: Persistence of symptoms, changing patterns over time\n",
       "5. **Interaction Features**: Combinations of symptoms, mental health, and utilization\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# AI-Driven Identification and Management of Somatic Symptom Disorder in Primary Care\n",
    "# Part 2: Feature Engineering and Data Integration\n",
    "\n",
    "display(Markdown(\"# Part 2: Feature Engineering and Data Integration\"))\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "## Research Questions and Hypothesis\n",
    "\n",
    "**Primary Research Question:**  \n",
    "Can we identify patients with probable Somatic Symptom Disorder (SSD) using EMR data from Canadian primary care practices?\n",
    "\n",
    "**Secondary Questions:**  \n",
    "1. What EMR features most strongly correlate with SSD patterns?\n",
    "2. How does healthcare utilization differ between patients with high vs. low somatic symptom burden?\n",
    "3. Can we identify clusters or subtypes within the SSD phenotype?\n",
    "\n",
    "**Hypothesis:**  \n",
    "We hypothesize that patients with SSD patterns will show:\n",
    "1. Higher frequency of primary care visits\n",
    "2. Multiple unexplained symptoms coded across body systems\n",
    "3. Higher proportion of normal lab results\n",
    "4. More frequent referrals to specialists\n",
    "5. Presence of mental health comorbidities (especially anxiety)\n",
    "6. Evidence of somatic language in clinical notes\n",
    "\"\"\"))\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "## Feature Engineering Plan\n",
    "\n",
    "We will transform the preprocessed data into a unified patient-level feature set for analysis.\n",
    "Our engineering approach includes:\n",
    "\n",
    "1. **Basic Utilization Features**: Visit counts, visit frequency, lab test counts\n",
    "2. **SSD-Specific Features**: SSD codes, unexplained symptom patterns, normal test ratios\n",
    "3. **Text-Derived Features**: Somatic language markers in clinical notes \n",
    "4. **Temporal Pattern Features**: Persistence of symptoms, changing patterns over time\n",
    "5. **Interaction Features**: Combinations of symptoms, mental health, and utilization\n",
    "\"\"\"))\n",
    "\n",
    "# Display brief summary of data we have so far\n",
    "tables_summary = pd.DataFrame(columns=['Table', 'Rows', 'Patients', 'Date Range'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_and_preprocess_cpcssn_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# First, we need to load our preprocessed data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Use the function from Part 1\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m load_and_preprocess_cpcssn_data(sample_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)  \u001b[38;5;66;03m# Using same sample size as in Part 1\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Summarize the data we've processed \u001b[39;00m\n\u001b[0;32m      6\u001b[0m tables_summary \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_and_preprocess_cpcssn_data' is not defined"
     ]
    }
   ],
   "source": [
    "# First, we need to load our preprocessed data\n",
    "# Use the function from Part 1\n",
    "data = load_and_preprocess_cpcssn_data(sample_size=1000)  # Using same sample size as in Part 1\n",
    "\n",
    "# Summarize the data we've processed \n",
    "tables_summary = []\n",
    "\n",
    "if 'patients' in data and len(data['patients']) > 0:\n",
    "    patient_count = len(data['patients'])\n",
    "    tables_summary.append({\n",
    "        'Table': 'Patient Demographics',\n",
    "        'Rows': patient_count,\n",
    "        'Patients': patient_count,\n",
    "        'Key Metrics': f\"Mean Age: {data['patients']['Age'].mean():.1f}, Female: {(data['patients']['Sex'] == 'FEMALE').mean():.1%}\"\n",
    "    })\n",
    "\n",
    "if 'encounter_stats' in data and len(data['encounter_stats']) > 0:\n",
    "    ec_df = data['encounter_stats']\n",
    "    tables_summary.append({\n",
    "        'Table': 'Encounters',\n",
    "        'Rows': '-',\n",
    "        'Patients': len(ec_df),\n",
    "        'Key Metrics': f\"Avg encounters/patient: {ec_df['encounter_count'].mean():.1f}, Symptom codes: {ec_df.get('symptom_code_count', pd.Series([0])).mean():.1f}\"\n",
    "    })\n",
    "\n",
    "if 'lab_stats' in data and len(data['lab_stats']) > 0:\n",
    "    lab_df = data['lab_stats']\n",
    "    tables_summary.append({\n",
    "        'Table': 'Lab Tests',\n",
    "        'Rows': '-',\n",
    "        'Patients': len(lab_df),\n",
    "        'Key Metrics': f\"Avg tests/patient: {lab_df['total_lab_tests'].mean():.1f}, Normal ratio: {lab_df['normal_test_ratio'].mean():.2f}\"\n",
    "    })\n",
    "\n",
    "if 'referral_stats' in data and len(data['referral_stats']) > 0:\n",
    "    ref_df = data['referral_stats']\n",
    "    tables_summary.append({\n",
    "        'Table': 'Referrals',\n",
    "        'Rows': '-',\n",
    "        'Patients': len(ref_df),\n",
    "        'Key Metrics': f\"Avg referrals/patient: {ref_df['total_referrals'].mean():.1f}\"\n",
    "    })\n",
    "\n",
    "if 'mental_health' in data and len(data['mental_health']) > 0:\n",
    "    mh_df = data['mental_health']\n",
    "    tables_summary.append({\n",
    "        'Table': 'Mental Health',\n",
    "        'Rows': '-',\n",
    "        'Patients': len(mh_df),\n",
    "        'Key Metrics': f\"With MH condition: {mh_df['has_mental_health_condition'].mean():.1%}, SSD codes: {mh_df['has_ssd_related_code'].mean():.1%}\"\n",
    "    })\n",
    "\n",
    "# Also check for procedure and text analysis data\n",
    "if 'procedure_stats' in data and len(data['procedure_stats']) > 0:\n",
    "    proc_df = data['procedure_stats']\n",
    "    tables_summary.append({\n",
    "        'Table': 'Medical Procedures',\n",
    "        'Rows': '-',\n",
    "        'Patients': len(proc_df),\n",
    "        'Key Metrics': f\"Diagnostic procedures/patient: {proc_df['diagnostic_procedures'].mean():.1f}\"\n",
    "    })\n",
    "\n",
    "if 'reason_text_analysis' in data and len(data['reason_text_analysis']) > 0:\n",
    "    text_df = data['reason_text_analysis']\n",
    "    tables_summary.append({\n",
    "        'Table': 'Text Analysis (Reason)',\n",
    "        'Rows': '-',\n",
    "        'Patients': len(text_df),\n",
    "        'Key Metrics': f\"Somatic language density: {text_df['somatic_language_density'].mean():.3f}\"\n",
    "    })\n",
    "\n",
    "tables_df = pd.DataFrame(tables_summary)\n",
    "display(Markdown(\"### Data Summary:\"))\n",
    "display(tables_df)\n",
    "\n",
    "# Begin feature engineering by creating a master patient feature dataframe\n",
    "display(Markdown(\"## Feature Engineering\"))\n",
    "display(Markdown(\"### 1. Creating Patient-Level Feature Matrix\"))\n",
    "\n",
    "# Start with demographics as the base\n",
    "if 'patients' in data and len(data['patients']) > 0:\n",
    "    patient_features = data['patients'][['Patient_ID', 'Age', 'Sex']].copy()\n",
    "    \n",
    "    # Convert Sex to binary for modeling purposes (will be revised if other genders present)\n",
    "    patient_features['is_female'] = (patient_features['Sex'] == 'FEMALE').astype(int)\n",
    "    \n",
    "    # Display progress\n",
    "    print(f\"Created base feature matrix with {len(patient_features)} patients\")\n",
    "    print(f\"Starting features: {list(patient_features.columns)}\")\n",
    "else:\n",
    "    print(\"Error: No patient data available to create feature matrix\")\n",
    "    patient_features = pd.DataFrame(columns=['Patient_ID'])\n",
    "\n",
    "# Add encounter features if available\n",
    "if 'encounter_stats' in data and len(data['encounter_stats']) > 0:\n",
    "    enc_df = data['encounter_stats']\n",
    "    enc_features = ['encounter_count']\n",
    "    \n",
    "    # Add symptom code features if present\n",
    "    if 'symptom_code_count' in enc_df.columns:\n",
    "        enc_features.append('symptom_code_count')\n",
    "    if 'symptom_encounter_ratio' in enc_df.columns:\n",
    "        enc_features.append('symptom_encounter_ratio')\n",
    "    \n",
    "    # Merge features\n",
    "    patient_features = pd.merge(\n",
    "        patient_features,\n",
    "        enc_df[['Patient_ID'] + enc_features],\n",
    "        on='Patient_ID',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill NAs for patients with no encounters\n",
    "    for col in enc_features:\n",
    "        patient_features[col] = patient_features[col].fillna(0)\n",
    "    \n",
    "    print(f\"Added encounter features: {enc_features}\")\n",
    "\n",
    "# Display first few rows of our evolving feature matrix\n",
    "display(Markdown(\"### Current Feature Matrix Preview:\"))\n",
    "display(patient_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
