#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
artefact_tracker.py â€“ Artefact Provenance Tracking Utility

This module provides functions to create and manage provenance metadata
for artefacts generated by the SSD analysis pipeline.

Each artefact gets a sidecar JSON file with:
- Script that generated it
- Timestamp
- Hypotheses supported
- Input data versions
- Key metrics
"""

import json
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any


def create_artefact_metadata(
    artefact_path: str | Path,
    script_name: str,
    hypotheses: List[str],
    input_files: Optional[Dict[str, str]] = None,
    metrics: Optional[Dict[str, Any]] = None,
    description: Optional[str] = None
) -> Path:
    """
    Create a metadata sidecar file for an artefact.
    
    Args:
        artefact_path: Path to the artefact file
        script_name: Name of the script that generated it
        hypotheses: List of hypotheses this artefact supports (e.g., ["H1", "H2", "RQ"])
        input_files: Dict of input file paths and their checksums
        metrics: Dict of key metrics (e.g., {"n_patients": 250025})
        description: Optional description of the artefact
        
    Returns:
        Path to the created metadata file
    """
    artefact_path = Path(artefact_path)
    metadata_path = artefact_path.with_suffix(artefact_path.suffix + '.metadata.json')
    
    # Calculate checksum of artefact
    if artefact_path.exists():
        with open(artefact_path, 'rb') as f:
            artefact_checksum = hashlib.md5(f.read()).hexdigest()
    else:
        artefact_checksum = None
    
    metadata = {
        "artefact": {
            "filename": artefact_path.name,
            "path": str(artefact_path),
            "checksum": artefact_checksum,
            "size_bytes": artefact_path.stat().st_size if artefact_path.exists() else None
        },
        "provenance": {
            "script": script_name,
            "timestamp": datetime.now().isoformat(),
            "hypotheses": hypotheses,
            "description": description
        },
        "inputs": input_files or {},
        "metrics": metrics or {}
    }
    
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2)
    
    return metadata_path


def read_artefact_metadata(artefact_path: str | Path) -> Dict:
    """
    Read metadata for an artefact.
    
    Args:
        artefact_path: Path to the artefact file
        
    Returns:
        Dict containing the metadata
    """
    artefact_path = Path(artefact_path)
    metadata_path = artefact_path.with_suffix(artefact_path.suffix + '.metadata.json')
    
    if not metadata_path.exists():
        raise FileNotFoundError(f"No metadata found for {artefact_path}")
    
    with open(metadata_path, 'r') as f:
        return json.load(f)


def get_file_checksum(filepath: str | Path) -> str:
    """Calculate MD5 checksum of a file."""
    filepath = Path(filepath)
    if not filepath.exists():
        return "FILE_NOT_FOUND"
    
    with open(filepath, 'rb') as f:
        return hashlib.md5(f.read()).hexdigest()


def track_pipeline_artefact(
    output_path: str | Path,
    script_file: str,
    hypotheses: List[str],
    cohort_path: Optional[str | Path] = None,
    exposure_path: Optional[str | Path] = None,
    **kwargs
) -> Path:
    """
    Convenience function for tracking artefacts in the SSD pipeline.
    
    Args:
        output_path: Path to the output artefact
        script_file: __file__ from the calling script
        hypotheses: List of hypotheses supported
        cohort_path: Path to cohort.parquet if used as input
        exposure_path: Path to exposure.parquet if used as input
        **kwargs: Additional metrics to track
        
    Returns:
        Path to the created metadata file
    """
    script_name = Path(script_file).name
    
    # Build input files dict
    input_files = {}
    if cohort_path and Path(cohort_path).exists():
        input_files["cohort.parquet"] = get_file_checksum(cohort_path)
    if exposure_path and Path(exposure_path).exists():
        input_files["exposure.parquet"] = get_file_checksum(exposure_path)
    
    # Extract metrics from kwargs
    metrics = {k: v for k, v in kwargs.items() if k not in ['description']}
    description = kwargs.get('description')
    
    return create_artefact_metadata(
        artefact_path=output_path,
        script_name=script_name,
        hypotheses=hypotheses,
        input_files=input_files,
        metrics=metrics,
        description=description
    )


# Example usage in a script:
if __name__ == "__main__":
    # Example of how to use in 01_cohort_builder.py:
    """
    from src.artefact_tracker import track_pipeline_artefact
    
    # After saving cohort.parquet:
    track_pipeline_artefact(
        output_path=OUT_FILE,
        script_file=__file__,
        hypotheses=["H1", "H2", "H3", "H4", "H5", "H6", "RQ"],
        n_patients=len(elig),
        n_excluded_age=n_excluded_age,
        n_excluded_span=n_excluded_span,
        description="Eligible cohort after applying inclusion/exclusion criteria"
    )
    """
    print("Artefact tracker module loaded. See example usage in comments.")